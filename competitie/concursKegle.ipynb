{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c509d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "data_train_samples = pd.read_csv('data/train_samples.txt', sep=\"\t\", header=None)\n",
    "data_train_labels = pd.read_csv('data/train_labels.txt', sep=\"\t\", header=None)\n",
    "\n",
    "\n",
    "data_validation_samples = pd.read_csv('data/validation_samples.txt', sep=\"\t\", header=None)\n",
    "data_validation_labels = pd.read_csv('data/validation_labels.txt', sep=\"\t\", header=None)\n",
    "\n",
    "\n",
    "data_test_samples = pd.read_csv('data/test_samples.txt', sep=\"\t\", header=None)\n",
    "\n",
    "# print(data_train_samples)\n",
    "# print(data_train_labels)\n",
    "# print(data_validation_samples)\n",
    "# print(data_validation_labels)\n",
    "# print(data_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc20e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(train_data, test_data, type=None):\n",
    "    scaler = None\n",
    "    if type == 'standard':\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "\n",
    "\n",
    "    elif type == 'l1':\n",
    "        scaler = preprocessing.Normalizer(norm='l1')\n",
    "\n",
    "    elif type == 'l2':\n",
    "        scaler = preprocessing.Normalizer(norm='l2')\n",
    "\n",
    "    if scaler is not None:\n",
    "        scaler.fit(train_data)\n",
    "        scaled_train_data = scaler.transform(train_data)\n",
    "        scaled_test_data = scaler.transform(test_data) \n",
    "        return (scaled_train_data, scaled_test_data)\n",
    "    else:\n",
    "        print(\"No scaling was performed. Raw data is returned.\")\n",
    "        return (train_data, test_data)\n",
    "\n",
    "    \n",
    "def accuracy_score(y_true, y_pred):\n",
    "    return (y_true == y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2065a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = data_train_samples[0]\n",
    "train_data = data_train_samples[1]\n",
    "train_labels = data_train_labels[1]\n",
    "\n",
    "# print(train_data)\n",
    "# print(train_ids)\n",
    "# print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd7a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ids = data_validation_samples[0]\n",
    "validation_data = data_validation_samples[1]\n",
    "validation_labels = data_validation_labels[1]\n",
    "\n",
    "# print(validation_ids)\n",
    "# print(validation_data)\n",
    "# print(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b27c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = data_test_samples[0]\n",
    "test_data = data_test_samples[1]\n",
    "\n",
    "# print(train_ids)\n",
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17fc9bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = train_labels.astype('int')\n",
    "yvalidation = validation_labels.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d706e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trval_data = pd.concat([train_data, validation_data])\n",
    "# print(trval_data)\n",
    "\n",
    "# trval_data = data_train_samples[1].append(data_validation_samples[1])\n",
    "# print(trval_data)\n",
    "\n",
    "# cv = CountVectorizer(max_features = 20000)\n",
    "\n",
    "# xtrval = cv.fit_transform(trval_data)\n",
    "# xtrval = xtrval.toarray()\n",
    "# print(xtrval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7111616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphaList = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008, 0.009, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 0.0009, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "aa1b76cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6832\n",
      "0.2 0.6846\n",
      "0.3 0.6886\n",
      "0.4 0.6882\n",
      "0.5 0.6886\n",
      "0.6 0.6868\n",
      "0.7 0.6878\n",
      "0.8 0.6874\n",
      "0.9 0.6866\n",
      "0.01 0.6692\n",
      "0.02 0.6746\n",
      "0.03 0.6774\n",
      "0.04 0.6782\n",
      "0.05 0.678\n",
      "0.06 0.6784\n",
      "0.07 0.68\n",
      "0.08 0.6806\n",
      "0.09 0.6822\n",
      "0.001 0.659\n",
      "0.002 0.663\n",
      "0.003 0.6638\n",
      "0.004 0.666\n",
      "0.005 0.6666\n",
      "0.006 0.6668\n",
      "0.007 0.6682\n",
      "0.008 0.6682\n",
      "0.009 0.669\n",
      "0.0001 0.6494\n",
      "0.0002 0.6512\n",
      "0.0003 0.6536\n",
      "0.0004 0.654\n",
      "0.0005 0.6546\n",
      "0.0006 0.6562\n",
      "0.0007 0.6574\n",
      "0.0008 0.6574\n",
      "0.0009 0.6584\n",
      "1 0.6854\n",
      "2 0.6874\n",
      "3 0.675\n",
      "4 0.6582\n",
      "5 0.6346\n",
      "6 0.6184\n",
      "7 0.6032\n",
      "8 0.588\n",
      "9 0.5724\n",
      "10 0.5588\n",
      "11 0.5486\n",
      "12 0.538\n",
      "13 0.5302\n",
      "14 0.5252\n",
      "15 0.5182\n",
      "16 0.5094\n",
      "17 0.5028\n",
      "18 0.4976\n",
      "19 0.4938\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "38c5a1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6822\n",
      "0.2 0.6816\n",
      "0.3 0.6832\n",
      "0.4 0.682\n",
      "0.5 0.6814\n",
      "0.6 0.6828\n",
      "0.7 0.684\n",
      "0.8 0.683\n",
      "0.9 0.6822\n",
      "0.01 0.676\n",
      "0.02 0.678\n",
      "0.03 0.679\n",
      "0.04 0.6796\n",
      "0.05 0.6796\n",
      "0.06 0.6806\n",
      "0.07 0.6808\n",
      "0.08 0.6816\n",
      "0.09 0.682\n",
      "0.001 0.6664\n",
      "0.002 0.668\n",
      "0.003 0.669\n",
      "0.004 0.671\n",
      "0.005 0.6716\n",
      "0.006 0.6732\n",
      "0.007 0.6748\n",
      "0.008 0.675\n",
      "0.009 0.6752\n",
      "0.0001 0.659\n",
      "0.0002 0.6604\n",
      "0.0003 0.6612\n",
      "0.0004 0.6634\n",
      "0.0005 0.6644\n",
      "0.0006 0.6646\n",
      "0.0007 0.6654\n",
      "0.0008 0.6654\n",
      "0.0009 0.6658\n",
      "1 0.6814\n",
      "2 0.6834\n",
      "3 0.6792\n",
      "4 0.6754\n",
      "5 0.6746\n",
      "6 0.6672\n",
      "7 0.6614\n",
      "8 0.6546\n",
      "9 0.6466\n",
      "10 0.6362\n",
      "11 0.6278\n",
      "12 0.618\n",
      "13 0.609\n",
      "14 0.6024\n",
      "15 0.5944\n",
      "16 0.5874\n",
      "17 0.5806\n",
      "18 0.572\n",
      "19 0.5646\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features = 10000)\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "707f33db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.669\n",
      "0.2 0.67\n",
      "0.3 0.6688\n",
      "0.4 0.6686\n",
      "0.5 0.6696\n",
      "0.6 0.6688\n",
      "0.7 0.6688\n",
      "0.8 0.6684\n",
      "0.9 0.6686\n",
      "0.01 0.6674\n",
      "0.02 0.6682\n",
      "0.03 0.668\n",
      "0.04 0.6682\n",
      "0.05 0.6682\n",
      "0.06 0.668\n",
      "0.07 0.6686\n",
      "0.08 0.6692\n",
      "0.09 0.6692\n",
      "0.001 0.6646\n",
      "0.002 0.6654\n",
      "0.003 0.6658\n",
      "0.004 0.6664\n",
      "0.005 0.6668\n",
      "0.006 0.6668\n",
      "0.007 0.667\n",
      "0.008 0.6672\n",
      "0.009 0.6672\n",
      "0.0001 0.6632\n",
      "0.0002 0.664\n",
      "0.0003 0.6644\n",
      "0.0004 0.6646\n",
      "0.0005 0.6646\n",
      "0.0006 0.6646\n",
      "0.0007 0.6646\n",
      "0.0008 0.6646\n",
      "0.0009 0.6646\n",
      "1 0.668\n",
      "2 0.665\n",
      "3 0.6654\n",
      "4 0.663\n",
      "5 0.667\n",
      "6 0.6658\n",
      "7 0.6632\n",
      "8 0.6624\n",
      "9 0.659\n",
      "10 0.6566\n",
      "11 0.6526\n",
      "12 0.652\n",
      "13 0.6494\n",
      "14 0.6448\n",
      "15 0.6404\n",
      "16 0.635\n",
      "17 0.6336\n",
      "18 0.6282\n",
      "19 0.6262\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features = 5000)\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a92dc213",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for alphaValue in alphaList:\n",
    "#     pr = Perceptron(alpha = alphaValue)\n",
    "#     pr.fit(xtrain_cv, ytrain)\n",
    "#     predictedPer = pr.predict(scaled_validation)\n",
    "#     print(alphaValue, accuracy_score(predictedPer, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9a025386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6778\n",
      "0.2 0.6808\n",
      "0.3 0.6774\n",
      "0.4 0.6726\n",
      "0.5 0.664\n",
      "0.6 0.6558\n",
      "0.7 0.6458\n",
      "0.8 0.6354\n",
      "0.9 0.626\n",
      "0.01 0.6578\n",
      "0.02 0.6636\n",
      "0.03 0.6672\n",
      "0.04 0.669\n",
      "0.05 0.672\n",
      "0.06 0.6742\n",
      "0.07 0.6764\n",
      "0.08 0.6756\n",
      "0.09 0.6776\n",
      "0.001 0.6424\n",
      "0.002 0.6464\n",
      "0.003 0.6472\n",
      "0.004 0.6504\n",
      "0.005 0.6514\n",
      "0.006 0.6536\n",
      "0.007 0.6548\n",
      "0.008 0.6562\n",
      "0.009 0.6574\n",
      "0.0001 0.6338\n",
      "0.0002 0.637\n",
      "0.0003 0.6388\n",
      "0.0004 0.639\n",
      "0.0005 0.6394\n",
      "0.0006 0.6406\n",
      "0.0007 0.6412\n",
      "0.0008 0.6416\n",
      "0.0009 0.6422\n",
      "1 0.6166\n",
      "2 0.548\n",
      "3 0.5144\n",
      "4 0.4944\n",
      "5 0.477\n",
      "6 0.466\n",
      "7 0.4582\n",
      "8 0.452\n",
      "9 0.4468\n",
      "10 0.4426\n",
      "11 0.4406\n",
      "12 0.4386\n",
      "13 0.4376\n",
      "14 0.4364\n",
      "15 0.435\n",
      "16 0.4342\n",
      "17 0.4328\n",
      "18 0.432\n",
      "19 0.4304\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(analyzer='word', norm='l2')\n",
    "xtrain = tfid.fit_transform(train_data)\n",
    "xvalidation = tfid.transform(validation_data)\n",
    "xtest = tfid.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "08d4ad3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6734\n",
      "0.2 0.6766\n",
      "0.3 0.6812\n",
      "0.4 0.6824\n",
      "0.5 0.6834\n",
      "0.6 0.6786\n",
      "0.7 0.6776\n",
      "0.8 0.6728\n",
      "0.9 0.6684\n",
      "0.01 0.6564\n",
      "0.02 0.6608\n",
      "0.03 0.6628\n",
      "0.04 0.6652\n",
      "0.05 0.667\n",
      "0.06 0.6706\n",
      "0.07 0.673\n",
      "0.08 0.6736\n",
      "0.09 0.6746\n",
      "0.001 0.642\n",
      "0.002 0.6456\n",
      "0.003 0.648\n",
      "0.004 0.6496\n",
      "0.005 0.6516\n",
      "0.006 0.6536\n",
      "0.007 0.655\n",
      "0.008 0.6538\n",
      "0.009 0.6552\n",
      "0.0001 0.631\n",
      "0.0002 0.6334\n",
      "0.0003 0.634\n",
      "0.0004 0.6348\n",
      "0.0005 0.6368\n",
      "0.0006 0.6376\n",
      "0.0007 0.6386\n",
      "0.0008 0.6388\n",
      "0.0009 0.641\n",
      "1 0.6664\n",
      "2 0.6142\n",
      "3 0.577\n",
      "4 0.55\n",
      "5 0.5362\n",
      "6 0.523\n",
      "7 0.513\n",
      "8 0.5044\n",
      "9 0.496\n",
      "10 0.489\n",
      "11 0.4826\n",
      "12 0.478\n",
      "13 0.4744\n",
      "14 0.4714\n",
      "15 0.4692\n",
      "16 0.467\n",
      "17 0.466\n",
      "18 0.4638\n",
      "19 0.4612\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(analyzer='word', norm='l2')\n",
    "xtrain = tfid.fit_transform(train_data)\n",
    "xvalidation = tfid.transform(validation_data)\n",
    "xtest = tfid.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue, fit_prior = False)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "aaff3bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.4758\n",
      "0.2 0.449\n",
      "0.3 0.4382\n",
      "0.4 0.4356\n",
      "0.5 0.4328\n",
      "0.6 0.4306\n",
      "0.7 0.43\n",
      "0.8 0.4296\n",
      "0.9 0.429\n",
      "0.01 0.5692\n",
      "0.02 0.5492\n",
      "0.03 0.5314\n",
      "0.04 0.5196\n",
      "0.05 0.5098\n",
      "0.06 0.5008\n",
      "0.07 0.4954\n",
      "0.08 0.4874\n",
      "0.09 0.4816\n",
      "0.001 0.5914\n",
      "0.002 0.5906\n",
      "0.003 0.5858\n",
      "0.004 0.5832\n",
      "0.005 0.58\n",
      "0.006 0.577\n",
      "0.007 0.5738\n",
      "0.008 0.5712\n",
      "0.009 0.5698\n",
      "0.0001 0.5962\n",
      "0.0002 0.5952\n",
      "0.0003 0.5962\n",
      "0.0004 0.5952\n",
      "0.0005 0.5942\n",
      "0.0006 0.5928\n",
      "0.0007 0.5924\n",
      "0.0008 0.5914\n",
      "0.0009 0.5912\n",
      "1 0.4286\n",
      "2 0.4268\n",
      "3 0.4238\n",
      "4 0.421\n",
      "5 0.4188\n",
      "6 0.4178\n",
      "7 0.4172\n",
      "8 0.415\n",
      "9 0.4112\n",
      "10 0.4084\n",
      "11 0.4076\n",
      "12 0.4068\n",
      "13 0.4048\n",
      "14 0.4032\n",
      "15 0.4026\n",
      "16 0.4018\n",
      "17 0.4014\n",
      "18 0.4014\n",
      "19 0.4014\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(analyzer='word', norm='l1')\n",
    "xtrain = tfid.fit_transform(train_data)\n",
    "xvalidation = tfid.transform(validation_data)\n",
    "xtest = tfid.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "9eec92aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnb2 = MultinomialNB(alpha = 0.03)\n",
    "# mnb2.fit(xtrain_cv, ytrain)\n",
    "# predictedMnbSubmission = mnb2.predict(xtest_cv)\n",
    "\n",
    "# output = open('data/test_labels.txt', 'w')\n",
    "# output.write('id,label\\n')\n",
    "# for i in range(len(test_ids)):\n",
    "#     output.write(str(test_ids[i]) + ',' + str(predictedMnbSubmission[i]) + '\\n')\n",
    "# output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3cea2087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.674\n",
      "0.2 0.6734\n",
      "0.3 0.6742\n",
      "0.4 0.6744\n",
      "0.5 0.6744\n",
      "0.6 0.6722\n",
      "0.7 0.6718\n",
      "0.8 0.6736\n",
      "0.9 0.6682\n",
      "0.01 0.6728\n",
      "0.02 0.673\n",
      "0.03 0.6726\n",
      "0.04 0.6728\n",
      "0.05 0.6726\n",
      "0.06 0.673\n",
      "0.07 0.6732\n",
      "0.08 0.6736\n",
      "0.09 0.6732\n",
      "0.001 0.6728\n",
      "0.002 0.6728\n",
      "0.003 0.6728\n",
      "0.004 0.6728\n",
      "0.005 0.673\n",
      "0.006 0.6728\n",
      "0.007 0.6728\n",
      "0.008 0.6728\n",
      "0.009 0.6728\n",
      "0.0001 0.6728\n",
      "0.0002 0.6728\n",
      "0.0003 0.6728\n",
      "0.0004 0.6728\n",
      "0.0005 0.6728\n",
      "0.0006 0.6728\n",
      "0.0007 0.6728\n",
      "0.0008 0.6728\n",
      "0.0009 0.6728\n",
      "1 0.6742\n",
      "2 0.6766\n",
      "3 0.6662\n",
      "4 0.6304\n",
      "5 0.6352\n",
      "6 0.5678\n",
      "7 0.6406\n",
      "8 0.6538\n",
      "9 0.6582\n",
      "10 0.5742\n",
      "11 0.6468\n",
      "12 0.6054\n",
      "13 0.5452\n",
      "14 0.6002\n",
      "15 0.6046\n",
      "16 0.6612\n",
      "17 0.6234\n",
      "18 0.6688\n",
      "19 0.6378\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(analyzer='word', norm='l2')\n",
    "xtrain = tfid.fit_transform(train_data)\n",
    "xvalidation = tfid.transform(validation_data)\n",
    "xtest = tfid.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    svc = LinearSVC(tol = alphaValue)\n",
    "    svc.fit(xtrain, ytrain)\n",
    "    predicted = svc.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "afc572f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6392\n",
      "0.2 0.6414\n",
      "0.3 0.6402\n",
      "0.4 0.6412\n",
      "0.5 0.641\n",
      "0.6 0.644\n",
      "0.7 0.6406\n",
      "0.8 0.644\n",
      "0.9 0.643\n",
      "0.01 0.6388\n",
      "0.02 0.638\n",
      "0.03 0.6382\n",
      "0.04 0.638\n",
      "0.05 0.6388\n",
      "0.06 0.6384\n",
      "0.07 0.64\n",
      "0.08 0.64\n",
      "0.09 0.6386\n",
      "0.001 0.6386\n",
      "0.002 0.6386\n",
      "0.003 0.6386\n",
      "0.004 0.6386\n",
      "0.005 0.6388\n",
      "0.006 0.6386\n",
      "0.007 0.6386\n",
      "0.008 0.6386\n",
      "0.009 0.6388\n",
      "0.0001 0.6386\n",
      "0.0002 0.6386\n",
      "0.0003 0.6386\n",
      "0.0004 0.6386\n",
      "0.0005 0.6386\n",
      "0.0006 0.6386\n",
      "0.0007 0.6386\n",
      "0.0008 0.6386\n",
      "0.0009 0.6386\n",
      "1 0.641\n",
      "2 0.6448\n",
      "3 0.6408\n",
      "4 0.6214\n",
      "5 0.6148\n",
      "6 0.6262\n",
      "7 0.6214\n",
      "8 0.616\n",
      "9 0.6214\n",
      "10 0.6188\n",
      "11 0.6272\n",
      "12 0.6174\n",
      "13 0.6266\n",
      "14 0.6278\n",
      "15 0.6302\n",
      "16 0.6242\n",
      "17 0.6314\n",
      "18 0.628\n",
      "19 0.6292\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(analyzer='word', norm='l1')\n",
    "xtrain = tfid.fit_transform(train_data)\n",
    "xvalidation = tfid.transform(validation_data)\n",
    "xtest = tfid.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    svc = LinearSVC(penalty = 'l1', loss = 'squared_hinge', dual = False, tol = alphaValue)\n",
    "    svc.fit(xtrain, ytrain)\n",
    "    predicted = svc.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e046331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6824\n",
      "0.2 0.682\n",
      "0.3 0.6788\n",
      "0.4 0.6786\n",
      "0.5 0.6804\n",
      "0.6 0.68\n",
      "0.7 0.6768\n",
      "0.8 0.6764\n",
      "0.9 0.675\n",
      "0.01 0.5932\n",
      "0.02 0.641\n",
      "0.03 0.6568\n",
      "0.04 0.6644\n",
      "0.05 0.6706\n",
      "0.06 0.6754\n",
      "0.07 0.678\n",
      "0.08 0.68\n",
      "0.09 0.6816\n",
      "0.001 0.4174\n",
      "0.002 0.4386\n",
      "0.003 0.4714\n",
      "0.004 0.4942\n",
      "0.005 0.525\n",
      "0.006 0.5472\n",
      "0.007 0.563\n",
      "0.008 0.5736\n",
      "0.009 0.5846\n",
      "0.0001 0.4\n",
      "0.0002 0.4\n",
      "0.0003 0.4\n",
      "0.0004 0.4\n",
      "0.0005 0.4002\n",
      "0.0006 0.4056\n",
      "0.0007 0.4114\n",
      "0.0008 0.4144\n",
      "0.0009 0.4166\n",
      "1 0.6728\n",
      "2 0.6624\n",
      "3 0.6552\n",
      "4 0.652\n",
      "5 0.6496\n",
      "6 0.6488\n",
      "7 0.6476\n",
      "8 0.6472\n",
      "9 0.6458\n",
      "10 0.6456\n",
      "11 0.6444\n",
      "12 0.644\n",
      "13 0.6434\n",
      "14 0.643\n",
      "15 0.6432\n",
      "16 0.6426\n",
      "17 0.6426\n",
      "18 0.6424\n",
      "19 0.6416\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(analyzer='word', norm='l2')\n",
    "xtrain = tfid.fit_transform(train_data)\n",
    "xvalidation = tfid.transform(validation_data)\n",
    "xtest = tfid.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    svc = LinearSVC(C = alphaValue)\n",
    "    svc.fit(xtrain, ytrain)\n",
    "    predicted = svc.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9d69e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.1 0.683\n",
      "0.2 0.1 0.682\n",
      "0.3 0.1 0.6796\n",
      "0.4 0.1 0.6792\n",
      "0.5 0.1 0.6804\n",
      "0.6 0.1 0.68\n",
      "0.7 0.1 0.6772\n",
      "0.8 0.1 0.6756\n",
      "0.9 0.1 0.6746\n",
      "0.01 0.1 0.5926\n",
      "0.02 0.1 0.641\n",
      "0.03 0.1 0.6568\n",
      "0.04 0.1 0.6644\n",
      "0.05 0.1 0.6708\n",
      "0.06 0.1 0.675\n",
      "0.07 0.1 0.678\n",
      "0.08 0.1 0.68\n",
      "0.09 0.1 0.681\n",
      "0.001 0.1 0.4174\n",
      "0.002 0.1 0.4386\n",
      "0.003 0.1 0.4712\n",
      "0.004 0.1 0.4942\n",
      "0.005 0.1 0.5246\n",
      "0.006 0.1 0.5468\n",
      "0.007 0.1 0.5626\n",
      "0.008 0.1 0.5736\n",
      "0.009 0.1 0.585\n",
      "0.0001 0.1 0.4\n",
      "0.0002 0.1 0.4\n",
      "0.0003 0.1 0.4\n",
      "0.0004 0.1 0.4\n",
      "0.0005 0.1 0.4002\n",
      "0.0006 0.1 0.4056\n",
      "0.0007 0.1 0.4114\n",
      "0.0008 0.1 0.4144\n",
      "0.0009 0.1 0.4166\n",
      "1 0.1 0.6734\n",
      "2 0.1 0.6624\n",
      "3 0.1 0.6546\n",
      "4 0.1 0.6514\n",
      "5 0.1 0.6496\n",
      "6 0.1 0.6488\n",
      "7 0.1 0.6478\n",
      "8 0.1 0.6474\n",
      "9 0.1 0.646\n",
      "10 0.1 0.646\n",
      "11 0.1 0.6442\n",
      "12 0.1 0.6442\n",
      "13 0.1 0.644\n",
      "14 0.1 0.6432\n",
      "15 0.1 0.6432\n",
      "16 0.1 0.643\n",
      "17 0.1 0.643\n",
      "18 0.1 0.6426\n",
      "19 0.1 0.6422\n",
      "0.1 0.2 0.683\n",
      "0.2 0.2 0.6808\n",
      "0.3 0.2 0.6796\n",
      "0.4 0.2 0.6784\n",
      "0.5 0.2 0.679\n",
      "0.6 0.2 0.68\n",
      "0.7 0.2 0.6782\n",
      "0.8 0.2 0.6764\n",
      "0.9 0.2 0.6746\n",
      "0.01 0.2 0.5936\n",
      "0.02 0.2 0.641\n",
      "0.03 0.2 0.6568\n",
      "0.04 0.2 0.6642\n",
      "0.05 0.2 0.671\n",
      "0.06 0.2 0.6746\n",
      "0.07 0.2 0.6778\n",
      "0.08 0.2 0.6804\n",
      "0.09 0.2 0.681\n",
      "0.001 0.2 0.4176\n",
      "0.002 0.2 0.439\n",
      "0.003 0.2 0.4722\n",
      "0.004 0.2 0.4946\n",
      "0.005 0.2 0.5238\n",
      "0.006 0.2 0.5466\n",
      "0.007 0.2 0.563\n",
      "0.008 0.2 0.5736\n",
      "0.009 0.2 0.5854\n",
      "0.0001 0.2 0.4\n",
      "0.0002 0.2 0.4\n",
      "0.0003 0.2 0.4\n",
      "0.0004 0.2 0.4\n",
      "0.0005 0.2 0.4\n",
      "0.0006 0.2 0.4054\n",
      "0.0007 0.2 0.4114\n",
      "0.0008 0.2 0.4144\n",
      "0.0009 0.2 0.4166\n",
      "1 0.2 0.674\n",
      "2 0.2 0.6624\n",
      "3 0.2 0.6538\n",
      "4 0.2 0.6506\n",
      "5 0.2 0.6498\n",
      "6 0.2 0.6504\n",
      "7 0.2 0.6484\n",
      "8 0.2 0.6474\n",
      "9 0.2 0.6464\n",
      "10 0.2 0.6448\n",
      "11 0.2 0.6456\n",
      "12 0.2 0.6444\n",
      "13 0.2 0.644\n",
      "14 0.2 0.6436\n",
      "15 0.2 0.6438\n",
      "16 0.2 0.6436\n",
      "17 0.2 0.6436\n",
      "18 0.2 0.6424\n",
      "19 0.2 0.6428\n",
      "0.1 0.3 0.6814\n",
      "0.2 0.3 0.6816\n",
      "0.3 0.3 0.6782\n",
      "0.4 0.3 0.6792\n",
      "0.5 0.3 0.6798\n",
      "0.6 0.3 0.6794\n",
      "0.7 0.3 0.6774\n",
      "0.8 0.3 0.6762\n",
      "0.9 0.3 0.6756\n",
      "0.01 0.3 0.5942\n",
      "0.02 0.3 0.6416\n",
      "0.03 0.3 0.6572\n",
      "0.04 0.3 0.6642\n",
      "0.05 0.3 0.6706\n",
      "0.06 0.3 0.6748\n",
      "0.07 0.3 0.6782\n",
      "0.08 0.3 0.6802\n",
      "0.09 0.3 0.6804\n",
      "0.001 0.3 0.4176\n",
      "0.002 0.3 0.439\n",
      "0.003 0.3 0.4722\n",
      "0.004 0.3 0.4956\n",
      "0.005 0.3 0.5228\n",
      "0.006 0.3 0.5474\n",
      "0.007 0.3 0.5626\n",
      "0.008 0.3 0.5736\n",
      "0.009 0.3 0.585\n",
      "0.0001 0.3 0.4\n",
      "0.0002 0.3 0.4\n",
      "0.0003 0.3 0.4\n",
      "0.0004 0.3 0.4\n",
      "0.0005 0.3 0.4002\n",
      "0.0006 0.3 0.4056\n",
      "0.0007 0.3 0.4114\n",
      "0.0008 0.3 0.4144\n",
      "0.0009 0.3 0.4166\n",
      "1 0.3 0.6734\n",
      "2 0.3 0.6622\n",
      "3 0.3 0.6548\n",
      "4 0.3 0.6518\n",
      "5 0.3 0.6496\n",
      "6 0.3 0.6488\n",
      "7 0.3 0.6472\n",
      "8 0.3 0.6482\n",
      "9 0.3 0.6468\n",
      "10 0.3 0.6456\n",
      "11 0.3 0.6454\n",
      "12 0.3 0.644\n",
      "13 0.3 0.644\n",
      "14 0.3 0.6438\n",
      "15 0.3 0.6424\n",
      "16 0.3 0.6434\n",
      "17 0.3 0.6448\n",
      "18 0.3 0.6432\n",
      "19 0.3 0.6428\n",
      "0.1 0.4 0.6824\n",
      "0.2 0.4 0.6804\n",
      "0.3 0.4 0.6798\n",
      "0.4 0.4 0.6786\n",
      "0.5 0.4 0.68\n",
      "0.6 0.4 0.6814\n",
      "0.7 0.4 0.6796\n",
      "0.8 0.4 0.6768\n",
      "0.9 0.4 0.6772\n",
      "0.01 0.4 0.5936\n",
      "0.02 0.4 0.6432\n",
      "0.03 0.4 0.6572\n",
      "0.04 0.4 0.664\n",
      "0.05 0.4 0.6702\n",
      "0.06 0.4 0.674\n",
      "0.07 0.4 0.6784\n",
      "0.08 0.4 0.6804\n",
      "0.09 0.4 0.6806\n",
      "0.001 0.4 0.4174\n",
      "0.002 0.4 0.439\n",
      "0.003 0.4 0.4728\n",
      "0.004 0.4 0.4954\n",
      "0.005 0.4 0.5274\n",
      "0.006 0.4 0.5476\n",
      "0.007 0.4 0.564\n",
      "0.008 0.4 0.5748\n",
      "0.009 0.4 0.5876\n",
      "0.0001 0.4 0.4\n",
      "0.0002 0.4 0.4\n",
      "0.0003 0.4 0.4\n",
      "0.0004 0.4 0.4\n",
      "0.0005 0.4 0.4\n",
      "0.0006 0.4 0.4066\n",
      "0.0007 0.4 0.4114\n",
      "0.0008 0.4 0.4144\n",
      "0.0009 0.4 0.4166\n",
      "1 0.4 0.6732\n",
      "2 0.4 0.6632\n",
      "3 0.4 0.6556\n",
      "4 0.4 0.651\n",
      "5 0.4 0.6494\n",
      "6 0.4 0.649\n",
      "7 0.4 0.6484\n",
      "8 0.4 0.6466\n",
      "9 0.4 0.6474\n",
      "10 0.4 0.6466\n",
      "11 0.4 0.6448\n",
      "12 0.4 0.644\n",
      "13 0.4 0.6448\n",
      "14 0.4 0.6438\n",
      "15 0.4 0.6452\n",
      "16 0.4 0.6436\n",
      "17 0.4 0.6434\n",
      "18 0.4 0.6438\n",
      "19 0.4 0.6432\n",
      "0.1 0.5 0.6828\n",
      "0.2 0.5 0.6814\n",
      "0.3 0.5 0.68\n",
      "0.4 0.5 0.6812\n",
      "0.5 0.5 0.6798\n",
      "0.6 0.5 0.6778\n",
      "0.7 0.5 0.677\n",
      "0.8 0.5 0.6772\n",
      "0.9 0.5 0.677\n",
      "0.01 0.5 0.594\n",
      "0.02 0.5 0.6408\n",
      "0.03 0.5 0.6572\n",
      "0.04 0.5 0.6644\n",
      "0.05 0.5 0.671\n",
      "0.06 0.5 0.6754\n",
      "0.07 0.5 0.6778\n",
      "0.08 0.5 0.6798\n",
      "0.09 0.5 0.681\n",
      "0.001 0.5 0.4176\n",
      "0.002 0.5 0.4392\n",
      "0.003 0.5 0.4722\n",
      "0.004 0.5 0.4938\n",
      "0.005 0.5 0.5254\n",
      "0.006 0.5 0.5504\n",
      "0.007 0.5 0.5628\n",
      "0.008 0.5 0.5738\n",
      "0.009 0.5 0.5854\n",
      "0.0001 0.5 0.4\n",
      "0.0002 0.5 0.4\n",
      "0.0003 0.5 0.4\n",
      "0.0004 0.5 0.4\n",
      "0.0005 0.5 0.4002\n",
      "0.0006 0.5 0.4054\n",
      "0.0007 0.5 0.4114\n",
      "0.0008 0.5 0.4144\n",
      "0.0009 0.5 0.4164\n",
      "1 0.5 0.6744\n",
      "2 0.5 0.6614\n",
      "3 0.5 0.6546\n",
      "4 0.5 0.6512\n",
      "5 0.5 0.65\n",
      "6 0.5 0.6488\n",
      "7 0.5 0.648\n",
      "8 0.5 0.6466\n",
      "9 0.5 0.6476\n",
      "10 0.5 0.6458\n",
      "11 0.5 0.6454\n",
      "12 0.5 0.6458\n",
      "13 0.5 0.6446\n",
      "14 0.5 0.644\n",
      "15 0.5 0.6448\n",
      "16 0.5 0.6434\n",
      "17 0.5 0.6432\n",
      "18 0.5 0.6434\n",
      "19 0.5 0.6436\n",
      "0.1 0.6 0.6826\n",
      "0.2 0.6 0.6804\n",
      "0.3 0.6 0.6794\n",
      "0.4 0.6 0.6782\n",
      "0.5 0.6 0.6782\n",
      "0.6 0.6 0.6786\n",
      "0.7 0.6 0.678\n",
      "0.8 0.6 0.6776\n",
      "0.9 0.6 0.6746\n",
      "0.01 0.6 0.5934\n",
      "0.02 0.6 0.6408\n",
      "0.03 0.6 0.6584\n",
      "0.04 0.6 0.6664\n",
      "0.05 0.6 0.6718\n",
      "0.06 0.6 0.6744\n",
      "0.07 0.6 0.6782\n",
      "0.08 0.6 0.6772\n",
      "0.09 0.6 0.6812\n",
      "0.001 0.6 0.4174\n",
      "0.002 0.6 0.4408\n",
      "0.003 0.6 0.4722\n",
      "0.004 0.6 0.4938\n",
      "0.005 0.6 0.5256\n",
      "0.006 0.6 0.5486\n",
      "0.007 0.6 0.5644\n",
      "0.008 0.6 0.574\n",
      "0.009 0.6 0.5856\n",
      "0.0001 0.6 0.4\n",
      "0.0002 0.6 0.4\n",
      "0.0003 0.6 0.4\n",
      "0.0004 0.6 0.4\n",
      "0.0005 0.6 0.4032\n",
      "0.0006 0.6 0.4134\n",
      "0.0007 0.6 0.4006\n",
      "0.0008 0.6 0.4154\n",
      "0.0009 0.6 0.416\n",
      "1 0.6 0.6736\n",
      "2 0.6 0.6632\n",
      "3 0.6 0.6562\n",
      "4 0.6 0.6502\n",
      "5 0.6 0.65\n",
      "6 0.6 0.6478\n",
      "7 0.6 0.6486\n",
      "8 0.6 0.6488\n",
      "9 0.6 0.6468\n",
      "10 0.6 0.643\n",
      "11 0.6 0.6452\n",
      "12 0.6 0.6432\n",
      "13 0.6 0.6458\n",
      "14 0.6 0.6438\n",
      "15 0.6 0.6452\n",
      "16 0.6 0.644\n",
      "17 0.6 0.6452\n",
      "18 0.6 0.6444\n",
      "19 0.6 0.6438\n",
      "0.1 0.7 0.6824\n",
      "0.2 0.7 0.6824\n",
      "0.3 0.7 0.6776\n",
      "0.4 0.7 0.6792\n",
      "0.5 0.7 0.6788\n",
      "0.6 0.7 0.6816\n",
      "0.7 0.7 0.6806\n",
      "0.8 0.7 0.6756\n",
      "0.9 0.7 0.6774\n",
      "0.01 0.7 0.5928\n",
      "0.02 0.7 0.6416\n",
      "0.03 0.7 0.6602\n",
      "0.04 0.7 0.662\n",
      "0.05 0.7 0.6692\n",
      "0.06 0.7 0.6738\n",
      "0.07 0.7 0.6794\n",
      "0.08 0.7 0.6814\n",
      "0.09 0.7 0.6804\n",
      "0.001 0.7 0.4214\n",
      "0.002 0.7 0.4356\n",
      "0.003 0.7 0.4714\n",
      "0.004 0.7 0.4952\n",
      "0.005 0.7 0.5242\n",
      "0.006 0.7 0.5484\n",
      "0.007 0.7 0.5624\n",
      "0.008 0.7 0.5732\n",
      "0.009 0.7 0.5848\n",
      "0.0001 0.7 0.4\n",
      "0.0002 0.7 0.4\n",
      "0.0003 0.7 0.4\n",
      "0.0004 0.7 0.4\n",
      "0.0005 0.7 0.4034\n",
      "0.0006 0.7 0.414\n",
      "0.0007 0.7 0.4196\n",
      "0.0008 0.7 0.4214\n",
      "0.0009 0.7 0.4238\n",
      "1 0.7 0.6742\n",
      "2 0.7 0.663\n",
      "3 0.7 0.6562\n",
      "4 0.7 0.6524\n",
      "5 0.7 0.6508\n",
      "6 0.7 0.6486\n",
      "7 0.7 0.6472\n",
      "8 0.7 0.6484\n",
      "9 0.7 0.6492\n",
      "10 0.7 0.6486\n",
      "11 0.7 0.6454\n",
      "12 0.7 0.6458\n",
      "13 0.7 0.645\n",
      "14 0.7 0.6444\n",
      "15 0.7 0.6436\n",
      "16 0.7 0.644\n",
      "17 0.7 0.6426\n",
      "18 0.7 0.6452\n",
      "19 0.7 0.6436\n",
      "0.1 0.8 0.6826\n",
      "0.2 0.8 0.6818\n",
      "0.3 0.8 0.6792\n",
      "0.4 0.8 0.6802\n",
      "0.5 0.8 0.6812\n",
      "0.6 0.8 0.6786\n",
      "0.7 0.8 0.6804\n",
      "0.8 0.8 0.676\n",
      "0.9 0.8 0.6714\n",
      "0.01 0.8 0.5928\n",
      "0.02 0.8 0.6408\n",
      "0.03 0.8 0.656\n",
      "0.04 0.8 0.6658\n",
      "0.05 0.8 0.671\n",
      "0.06 0.8 0.6738\n",
      "0.07 0.8 0.676\n",
      "0.08 0.8 0.682\n",
      "0.09 0.8 0.6796\n",
      "0.001 0.8 0.42\n",
      "0.002 0.8 0.4384\n",
      "0.003 0.8 0.4712\n",
      "0.004 0.8 0.499\n",
      "0.005 0.8 0.5258\n",
      "0.006 0.8 0.5488\n",
      "0.007 0.8 0.5634\n",
      "0.008 0.8 0.575\n",
      "0.009 0.8 0.585\n",
      "0.0001 0.8 0.4\n",
      "0.0002 0.8 0.4\n",
      "0.0003 0.8 0.4\n",
      "0.0004 0.8 0.4\n",
      "0.0005 0.8 0.4166\n",
      "0.0006 0.8 0.4102\n",
      "0.0007 0.8 0.4154\n",
      "0.0008 0.8 0.4218\n",
      "0.0009 0.8 0.421\n",
      "1 0.8 0.6716\n",
      "2 0.8 0.6614\n",
      "3 0.8 0.6558\n",
      "4 0.8 0.651\n",
      "5 0.8 0.65\n",
      "6 0.8 0.6454\n",
      "7 0.8 0.648\n",
      "8 0.8 0.6478\n",
      "9 0.8 0.6466\n",
      "10 0.8 0.6448\n",
      "11 0.8 0.6502\n",
      "12 0.8 0.646\n",
      "13 0.8 0.6464\n",
      "14 0.8 0.6436\n",
      "15 0.8 0.6448\n",
      "16 0.8 0.6446\n",
      "17 0.8 0.6452\n",
      "18 0.8 0.6424\n",
      "19 0.8 0.6446\n",
      "0.1 0.9 0.682\n",
      "0.2 0.9 0.682\n",
      "0.3 0.9 0.6798\n",
      "0.4 0.9 0.6784\n",
      "0.5 0.9 0.6802\n",
      "0.6 0.9 0.6776\n",
      "0.7 0.9 0.6758\n",
      "0.8 0.9 0.679\n",
      "0.9 0.9 0.676\n",
      "0.01 0.9 0.5932\n",
      "0.02 0.9 0.6406\n",
      "0.03 0.9 0.6588\n",
      "0.04 0.9 0.6646\n",
      "0.05 0.9 0.6682\n",
      "0.06 0.9 0.6732\n",
      "0.07 0.9 0.6722\n",
      "0.08 0.9 0.6812\n",
      "0.09 0.9 0.6836\n",
      "0.001 0.9 0.4292\n",
      "0.002 0.9 0.47\n",
      "0.003 0.9 0.4692\n",
      "0.004 0.9 0.5\n",
      "0.005 0.9 0.5204\n",
      "0.006 0.9 0.548\n",
      "0.007 0.9 0.5636\n",
      "0.008 0.9 0.5766\n",
      "0.009 0.9 0.5852\n",
      "0.0001 0.9 0.4\n",
      "0.0002 0.9 0.4\n",
      "0.0003 0.9 0.4\n",
      "0.0004 0.9 0.4004\n",
      "0.0005 0.9 0.4016\n",
      "0.0006 0.9 0.4126\n",
      "0.0007 0.9 0.4166\n",
      "0.0008 0.9 0.4194\n",
      "0.0009 0.9 0.4166\n",
      "1 0.9 0.6754\n",
      "2 0.9 0.6632\n",
      "3 0.9 0.6592\n",
      "4 0.9 0.652\n",
      "5 0.9 0.6512\n",
      "6 0.9 0.6488\n",
      "7 0.9 0.6464\n",
      "8 0.9 0.6496\n",
      "9 0.9 0.6476\n",
      "10 0.9 0.6484\n",
      "11 0.9 0.647\n",
      "12 0.9 0.6426\n",
      "13 0.9 0.6442\n",
      "14 0.9 0.6442\n",
      "15 0.9 0.6456\n",
      "16 0.9 0.6446\n",
      "17 0.9 0.6432\n",
      "18 0.9 0.644\n",
      "19 0.9 0.6454\n",
      "0.1 0.01 0.6824\n",
      "0.2 0.01 0.682\n",
      "0.3 0.01 0.6788\n",
      "0.4 0.01 0.6788\n",
      "0.5 0.01 0.6804\n",
      "0.6 0.01 0.68\n",
      "0.7 0.01 0.6766\n",
      "0.8 0.01 0.6764\n",
      "0.9 0.01 0.6748\n",
      "0.01 0.01 0.5934\n",
      "0.02 0.01 0.641\n",
      "0.03 0.01 0.6568\n",
      "0.04 0.01 0.6644\n",
      "0.05 0.01 0.6706\n",
      "0.06 0.01 0.6754\n",
      "0.07 0.01 0.678\n",
      "0.08 0.01 0.68\n",
      "0.09 0.01 0.6816\n",
      "0.001 0.01 0.4174\n",
      "0.002 0.01 0.4386\n",
      "0.003 0.01 0.4714\n",
      "0.004 0.01 0.4942\n",
      "0.005 0.01 0.525\n",
      "0.006 0.01 0.5472\n",
      "0.007 0.01 0.563\n",
      "0.008 0.01 0.5736\n",
      "0.009 0.01 0.5846\n",
      "0.0001 0.01 0.4\n",
      "0.0002 0.01 0.4\n",
      "0.0003 0.01 0.4\n",
      "0.0004 0.01 0.4\n",
      "0.0005 0.01 0.4002\n",
      "0.0006 0.01 0.4056\n",
      "0.0007 0.01 0.4114\n",
      "0.0008 0.01 0.4144\n",
      "0.0009 0.01 0.4166\n",
      "1 0.01 0.6728\n",
      "2 0.01 0.6624\n",
      "3 0.01 0.6552\n",
      "4 0.01 0.6518\n",
      "5 0.01 0.6494\n",
      "6 0.01 0.649\n",
      "7 0.01 0.6476\n",
      "8 0.01 0.647\n",
      "9 0.01 0.6458\n",
      "10 0.01 0.6456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0.01 0.6444\n",
      "12 0.01 0.644\n",
      "13 0.01 0.6434\n",
      "14 0.01 0.643\n",
      "15 0.01 0.6432\n",
      "16 0.01 0.6426\n",
      "17 0.01 0.6426\n",
      "18 0.01 0.6422\n",
      "19 0.01 0.6416\n",
      "0.1 0.02 0.6824\n",
      "0.2 0.02 0.682\n",
      "0.3 0.02 0.6786\n",
      "0.4 0.02 0.6786\n",
      "0.5 0.02 0.6802\n",
      "0.6 0.02 0.6796\n",
      "0.7 0.02 0.6768\n",
      "0.8 0.02 0.6764\n",
      "0.9 0.02 0.6744\n",
      "0.01 0.02 0.5932\n",
      "0.02 0.02 0.6412\n",
      "0.03 0.02 0.6568\n",
      "0.04 0.02 0.6644\n",
      "0.05 0.02 0.6706\n",
      "0.06 0.02 0.6754\n",
      "0.07 0.02 0.6782\n",
      "0.08 0.02 0.68\n",
      "0.09 0.02 0.6816\n",
      "0.001 0.02 0.4174\n",
      "0.002 0.02 0.4386\n",
      "0.003 0.02 0.4714\n",
      "0.004 0.02 0.4944\n",
      "0.005 0.02 0.525\n",
      "0.006 0.02 0.5472\n",
      "0.007 0.02 0.563\n",
      "0.008 0.02 0.5734\n",
      "0.009 0.02 0.5846\n",
      "0.0001 0.02 0.4\n",
      "0.0002 0.02 0.4\n",
      "0.0003 0.02 0.4\n",
      "0.0004 0.02 0.4\n",
      "0.0005 0.02 0.4002\n",
      "0.0006 0.02 0.4056\n",
      "0.0007 0.02 0.4114\n",
      "0.0008 0.02 0.4144\n",
      "0.0009 0.02 0.4166\n",
      "1 0.02 0.6728\n",
      "2 0.02 0.6624\n",
      "3 0.02 0.6554\n",
      "4 0.02 0.6518\n",
      "5 0.02 0.6496\n",
      "6 0.02 0.6488\n",
      "7 0.02 0.6476\n",
      "8 0.02 0.647\n",
      "9 0.02 0.646\n",
      "10 0.02 0.6456\n",
      "11 0.02 0.6444\n",
      "12 0.02 0.644\n",
      "13 0.02 0.6434\n",
      "14 0.02 0.6428\n",
      "15 0.02 0.6428\n",
      "16 0.02 0.6426\n",
      "17 0.02 0.6428\n",
      "18 0.02 0.6422\n",
      "19 0.02 0.6416\n",
      "0.1 0.03 0.6824\n",
      "0.2 0.03 0.6818\n",
      "0.3 0.03 0.6786\n",
      "0.4 0.03 0.6788\n",
      "0.5 0.03 0.6802\n",
      "0.6 0.03 0.6798\n",
      "0.7 0.03 0.6768\n",
      "0.8 0.03 0.677\n",
      "0.9 0.03 0.6744\n",
      "0.01 0.03 0.5934\n",
      "0.02 0.03 0.641\n",
      "0.03 0.03 0.657\n",
      "0.04 0.03 0.6644\n",
      "0.05 0.03 0.6706\n",
      "0.06 0.03 0.6752\n",
      "0.07 0.03 0.678\n",
      "0.08 0.03 0.68\n",
      "0.09 0.03 0.6814\n",
      "0.001 0.03 0.4174\n",
      "0.002 0.03 0.4386\n",
      "0.003 0.03 0.4714\n",
      "0.004 0.03 0.4942\n",
      "0.005 0.03 0.525\n",
      "0.006 0.03 0.547\n",
      "0.007 0.03 0.563\n",
      "0.008 0.03 0.5736\n",
      "0.009 0.03 0.5846\n",
      "0.0001 0.03 0.4\n",
      "0.0002 0.03 0.4\n",
      "0.0003 0.03 0.4\n",
      "0.0004 0.03 0.4\n",
      "0.0005 0.03 0.4002\n",
      "0.0006 0.03 0.4056\n",
      "0.0007 0.03 0.4114\n",
      "0.0008 0.03 0.4144\n",
      "0.0009 0.03 0.4166\n",
      "1 0.03 0.6726\n",
      "2 0.03 0.6624\n",
      "3 0.03 0.655\n",
      "4 0.03 0.6522\n",
      "5 0.03 0.6494\n",
      "6 0.03 0.6488\n",
      "7 0.03 0.648\n",
      "8 0.03 0.647\n",
      "9 0.03 0.646\n",
      "10 0.03 0.6452\n",
      "11 0.03 0.6444\n",
      "12 0.03 0.6436\n",
      "13 0.03 0.6434\n",
      "14 0.03 0.643\n",
      "15 0.03 0.643\n",
      "16 0.03 0.6428\n",
      "17 0.03 0.6428\n",
      "18 0.03 0.642\n",
      "19 0.03 0.642\n",
      "0.1 0.04 0.6824\n",
      "0.2 0.04 0.682\n",
      "0.3 0.04 0.6792\n",
      "0.4 0.04 0.6792\n",
      "0.5 0.04 0.68\n",
      "0.6 0.04 0.6798\n",
      "0.7 0.04 0.6766\n",
      "0.8 0.04 0.6766\n",
      "0.9 0.04 0.6746\n",
      "0.01 0.04 0.5932\n",
      "0.02 0.04 0.6408\n",
      "0.03 0.04 0.6568\n",
      "0.04 0.04 0.6644\n",
      "0.05 0.04 0.6706\n",
      "0.06 0.04 0.6752\n",
      "0.07 0.04 0.6782\n",
      "0.08 0.04 0.6802\n",
      "0.09 0.04 0.6816\n",
      "0.001 0.04 0.4174\n",
      "0.002 0.04 0.4386\n",
      "0.003 0.04 0.4714\n",
      "0.004 0.04 0.4942\n",
      "0.005 0.04 0.5248\n",
      "0.006 0.04 0.547\n",
      "0.007 0.04 0.563\n",
      "0.008 0.04 0.5734\n",
      "0.009 0.04 0.5844\n",
      "0.0001 0.04 0.4\n",
      "0.0002 0.04 0.4\n",
      "0.0003 0.04 0.4\n",
      "0.0004 0.04 0.4\n",
      "0.0005 0.04 0.4002\n",
      "0.0006 0.04 0.4056\n",
      "0.0007 0.04 0.4114\n",
      "0.0008 0.04 0.4144\n",
      "0.0009 0.04 0.4166\n",
      "1 0.04 0.6728\n",
      "2 0.04 0.6626\n",
      "3 0.04 0.6554\n",
      "4 0.04 0.652\n",
      "5 0.04 0.6494\n",
      "6 0.04 0.6492\n",
      "7 0.04 0.6476\n",
      "8 0.04 0.647\n",
      "9 0.04 0.646\n",
      "10 0.04 0.6454\n",
      "11 0.04 0.6442\n",
      "12 0.04 0.644\n",
      "13 0.04 0.6436\n",
      "14 0.04 0.643\n",
      "15 0.04 0.643\n",
      "16 0.04 0.6428\n",
      "17 0.04 0.643\n",
      "18 0.04 0.642\n",
      "19 0.04 0.6422\n",
      "0.1 0.05 0.6826\n",
      "0.2 0.05 0.6818\n",
      "0.3 0.05 0.679\n",
      "0.4 0.05 0.6794\n",
      "0.5 0.05 0.6802\n",
      "0.6 0.05 0.6796\n",
      "0.7 0.05 0.6772\n",
      "0.8 0.05 0.6766\n",
      "0.9 0.05 0.6752\n",
      "0.01 0.05 0.593\n",
      "0.02 0.05 0.641\n",
      "0.03 0.05 0.6568\n",
      "0.04 0.05 0.6644\n",
      "0.05 0.05 0.6706\n",
      "0.06 0.05 0.6754\n",
      "0.07 0.05 0.6782\n",
      "0.08 0.05 0.6798\n",
      "0.09 0.05 0.6816\n",
      "0.001 0.05 0.4174\n",
      "0.002 0.05 0.4386\n",
      "0.003 0.05 0.4712\n",
      "0.004 0.05 0.4944\n",
      "0.005 0.05 0.5248\n",
      "0.006 0.05 0.547\n",
      "0.007 0.05 0.563\n",
      "0.008 0.05 0.5734\n",
      "0.009 0.05 0.5846\n",
      "0.0001 0.05 0.4\n",
      "0.0002 0.05 0.4\n",
      "0.0003 0.05 0.4\n",
      "0.0004 0.05 0.4\n",
      "0.0005 0.05 0.4002\n",
      "0.0006 0.05 0.4056\n",
      "0.0007 0.05 0.4114\n",
      "0.0008 0.05 0.4144\n",
      "0.0009 0.05 0.4166\n",
      "1 0.05 0.6732\n",
      "2 0.05 0.6626\n",
      "3 0.05 0.6552\n",
      "4 0.05 0.6522\n",
      "5 0.05 0.6502\n",
      "6 0.05 0.649\n",
      "7 0.05 0.6478\n",
      "8 0.05 0.647\n",
      "9 0.05 0.6462\n",
      "10 0.05 0.6452\n",
      "11 0.05 0.6442\n",
      "12 0.05 0.6446\n",
      "13 0.05 0.6434\n",
      "14 0.05 0.6428\n",
      "15 0.05 0.6432\n",
      "16 0.05 0.6428\n",
      "17 0.05 0.6432\n",
      "18 0.05 0.642\n",
      "19 0.05 0.642\n",
      "0.1 0.06 0.6828\n",
      "0.2 0.06 0.6814\n",
      "0.3 0.06 0.6796\n",
      "0.4 0.06 0.679\n",
      "0.5 0.06 0.68\n",
      "0.6 0.06 0.68\n",
      "0.7 0.06 0.6772\n",
      "0.8 0.06 0.6768\n",
      "0.9 0.06 0.6748\n",
      "0.01 0.06 0.5932\n",
      "0.02 0.06 0.641\n",
      "0.03 0.06 0.6572\n",
      "0.04 0.06 0.6642\n",
      "0.05 0.06 0.6706\n",
      "0.06 0.06 0.6748\n",
      "0.07 0.06 0.6782\n",
      "0.08 0.06 0.6798\n",
      "0.09 0.06 0.681\n",
      "0.001 0.06 0.4176\n",
      "0.002 0.06 0.4386\n",
      "0.003 0.06 0.4712\n",
      "0.004 0.06 0.494\n",
      "0.005 0.06 0.5238\n",
      "0.006 0.06 0.5468\n",
      "0.007 0.06 0.5626\n",
      "0.008 0.06 0.5736\n",
      "0.009 0.06 0.5848\n",
      "0.0001 0.06 0.4\n",
      "0.0002 0.06 0.4\n",
      "0.0003 0.06 0.4\n",
      "0.0004 0.06 0.4\n",
      "0.0005 0.06 0.4002\n",
      "0.0006 0.06 0.4056\n",
      "0.0007 0.06 0.4114\n",
      "0.0008 0.06 0.4144\n",
      "0.0009 0.06 0.4166\n",
      "1 0.06 0.6726\n",
      "2 0.06 0.6622\n",
      "3 0.06 0.6552\n",
      "4 0.06 0.6518\n",
      "5 0.06 0.649\n",
      "6 0.06 0.6488\n",
      "7 0.06 0.648\n",
      "8 0.06 0.6472\n",
      "9 0.06 0.6462\n",
      "10 0.06 0.6456\n",
      "11 0.06 0.6444\n",
      "12 0.06 0.6442\n",
      "13 0.06 0.6436\n",
      "14 0.06 0.643\n",
      "15 0.06 0.6432\n",
      "16 0.06 0.643\n",
      "17 0.06 0.643\n",
      "18 0.06 0.6424\n",
      "19 0.06 0.6418\n",
      "0.1 0.07 0.6824\n",
      "0.2 0.07 0.6818\n",
      "0.3 0.07 0.679\n",
      "0.4 0.07 0.6794\n",
      "0.5 0.07 0.6796\n",
      "0.6 0.07 0.6798\n",
      "0.7 0.07 0.677\n",
      "0.8 0.07 0.6764\n",
      "0.9 0.07 0.675\n",
      "0.01 0.07 0.5928\n",
      "0.02 0.07 0.641\n",
      "0.03 0.07 0.6572\n",
      "0.04 0.07 0.6644\n",
      "0.05 0.07 0.6708\n",
      "0.06 0.07 0.6748\n",
      "0.07 0.07 0.6784\n",
      "0.08 0.07 0.6804\n",
      "0.09 0.07 0.6814\n",
      "0.001 0.07 0.4176\n",
      "0.002 0.07 0.4386\n",
      "0.003 0.07 0.4714\n",
      "0.004 0.07 0.494\n",
      "0.005 0.07 0.5242\n",
      "0.006 0.07 0.5472\n",
      "0.007 0.07 0.563\n",
      "0.008 0.07 0.5738\n",
      "0.009 0.07 0.5846\n",
      "0.0001 0.07 0.4\n",
      "0.0002 0.07 0.4\n",
      "0.0003 0.07 0.4\n",
      "0.0004 0.07 0.4\n",
      "0.0005 0.07 0.4002\n",
      "0.0006 0.07 0.4056\n",
      "0.0007 0.07 0.4114\n",
      "0.0008 0.07 0.4146\n",
      "0.0009 0.07 0.4166\n",
      "1 0.07 0.673\n",
      "2 0.07 0.6628\n",
      "3 0.07 0.6552\n",
      "4 0.07 0.6514\n",
      "5 0.07 0.6494\n",
      "6 0.07 0.6492\n",
      "7 0.07 0.6482\n",
      "8 0.07 0.647\n",
      "9 0.07 0.646\n",
      "10 0.07 0.6454\n",
      "11 0.07 0.6444\n",
      "12 0.07 0.644\n",
      "13 0.07 0.6436\n",
      "14 0.07 0.6428\n",
      "15 0.07 0.643\n",
      "16 0.07 0.6428\n",
      "17 0.07 0.6432\n",
      "18 0.07 0.642\n",
      "19 0.07 0.6422\n",
      "0.1 0.08 0.6824\n",
      "0.2 0.08 0.682\n",
      "0.3 0.08 0.6792\n",
      "0.4 0.08 0.6792\n",
      "0.5 0.08 0.681\n",
      "0.6 0.08 0.6804\n",
      "0.7 0.08 0.6766\n",
      "0.8 0.08 0.677\n",
      "0.9 0.08 0.675\n",
      "0.01 0.08 0.5932\n",
      "0.02 0.08 0.641\n",
      "0.03 0.08 0.6568\n",
      "0.04 0.08 0.6644\n",
      "0.05 0.08 0.6708\n",
      "0.06 0.08 0.675\n",
      "0.07 0.08 0.6782\n",
      "0.08 0.08 0.6802\n",
      "0.09 0.08 0.6814\n",
      "0.001 0.08 0.4176\n",
      "0.002 0.08 0.439\n",
      "0.003 0.08 0.4712\n",
      "0.004 0.08 0.4942\n",
      "0.005 0.08 0.5244\n",
      "0.006 0.08 0.5474\n",
      "0.007 0.08 0.563\n",
      "0.008 0.08 0.5736\n",
      "0.009 0.08 0.5852\n",
      "0.0001 0.08 0.4\n",
      "0.0002 0.08 0.4\n",
      "0.0003 0.08 0.4\n",
      "0.0004 0.08 0.4\n",
      "0.0005 0.08 0.4002\n",
      "0.0006 0.08 0.4054\n",
      "0.0007 0.08 0.4114\n",
      "0.0008 0.08 0.4146\n",
      "0.0009 0.08 0.4166\n",
      "1 0.08 0.6732\n",
      "2 0.08 0.6626\n",
      "3 0.08 0.6552\n",
      "4 0.08 0.6516\n",
      "5 0.08 0.6502\n",
      "6 0.08 0.6488\n",
      "7 0.08 0.648\n",
      "8 0.08 0.6476\n",
      "9 0.08 0.6462\n",
      "10 0.08 0.6452\n",
      "11 0.08 0.6444\n",
      "12 0.08 0.644\n",
      "13 0.08 0.6436\n",
      "14 0.08 0.643\n",
      "15 0.08 0.6434\n",
      "16 0.08 0.643\n",
      "17 0.08 0.6432\n",
      "18 0.08 0.642\n",
      "19 0.08 0.6422\n",
      "0.1 0.09 0.683\n",
      "0.2 0.09 0.6816\n",
      "0.3 0.09 0.68\n",
      "0.4 0.09 0.6792\n",
      "0.5 0.09 0.6808\n",
      "0.6 0.09 0.68\n",
      "0.7 0.09 0.6774\n",
      "0.8 0.09 0.6762\n",
      "0.9 0.09 0.6746\n",
      "0.01 0.09 0.592\n",
      "0.02 0.09 0.6408\n",
      "0.03 0.09 0.6572\n",
      "0.04 0.09 0.6644\n",
      "0.05 0.09 0.6708\n",
      "0.06 0.09 0.6752\n",
      "0.07 0.09 0.6782\n",
      "0.08 0.09 0.6796\n",
      "0.09 0.09 0.6808\n",
      "0.001 0.09 0.4176\n",
      "0.002 0.09 0.4386\n",
      "0.003 0.09 0.4712\n",
      "0.004 0.09 0.494\n",
      "0.005 0.09 0.5248\n",
      "0.006 0.09 0.5478\n",
      "0.007 0.09 0.5626\n",
      "0.008 0.09 0.5736\n",
      "0.009 0.09 0.5852\n",
      "0.0001 0.09 0.4\n",
      "0.0002 0.09 0.4\n",
      "0.0003 0.09 0.4\n",
      "0.0004 0.09 0.4\n",
      "0.0005 0.09 0.4002\n",
      "0.0006 0.09 0.4054\n",
      "0.0007 0.09 0.4114\n",
      "0.0008 0.09 0.4144\n",
      "0.0009 0.09 0.4166\n",
      "1 0.09 0.6736\n",
      "2 0.09 0.662\n",
      "3 0.09 0.6554\n",
      "4 0.09 0.6514\n",
      "5 0.09 0.6494\n",
      "6 0.09 0.6484\n",
      "7 0.09 0.6482\n",
      "8 0.09 0.647\n",
      "9 0.09 0.6462\n",
      "10 0.09 0.6454\n",
      "11 0.09 0.644\n",
      "12 0.09 0.644\n",
      "13 0.09 0.6436\n",
      "14 0.09 0.6432\n",
      "15 0.09 0.6434\n",
      "16 0.09 0.643\n",
      "17 0.09 0.6434\n",
      "18 0.09 0.6424\n",
      "19 0.09 0.642\n",
      "0.1 0.001 0.6824\n",
      "0.2 0.001 0.682\n",
      "0.3 0.001 0.6788\n",
      "0.4 0.001 0.6786\n",
      "0.5 0.001 0.6804\n",
      "0.6 0.001 0.68\n",
      "0.7 0.001 0.6768\n",
      "0.8 0.001 0.6764\n",
      "0.9 0.001 0.675\n",
      "0.01 0.001 0.5932\n",
      "0.02 0.001 0.641\n",
      "0.03 0.001 0.6568\n",
      "0.04 0.001 0.6644\n",
      "0.05 0.001 0.6706\n",
      "0.06 0.001 0.6754\n",
      "0.07 0.001 0.678\n",
      "0.08 0.001 0.68\n",
      "0.09 0.001 0.6816\n",
      "0.001 0.001 0.4176\n",
      "0.002 0.001 0.4386\n",
      "0.003 0.001 0.4714\n",
      "0.004 0.001 0.4942\n",
      "0.005 0.001 0.525\n",
      "0.006 0.001 0.5472\n",
      "0.007 0.001 0.563\n",
      "0.008 0.001 0.5736\n",
      "0.009 0.001 0.5846\n",
      "0.0001 0.001 0.4\n",
      "0.0002 0.001 0.4\n",
      "0.0003 0.001 0.4\n",
      "0.0004 0.001 0.4\n",
      "0.0005 0.001 0.4002\n",
      "0.0006 0.001 0.4056\n",
      "0.0007 0.001 0.4114\n",
      "0.0008 0.001 0.4144\n",
      "0.0009 0.001 0.4166\n",
      "1 0.001 0.6728\n",
      "2 0.001 0.6624\n",
      "3 0.001 0.6552\n",
      "4 0.001 0.652\n",
      "5 0.001 0.6496\n",
      "6 0.001 0.6488\n",
      "7 0.001 0.6476\n",
      "8 0.001 0.6472\n",
      "9 0.001 0.6458\n",
      "10 0.001 0.6456\n",
      "11 0.001 0.6444\n",
      "12 0.001 0.644\n",
      "13 0.001 0.6434\n",
      "14 0.001 0.643\n",
      "15 0.001 0.6432\n",
      "16 0.001 0.6426\n",
      "17 0.001 0.6426\n",
      "18 0.001 0.6424\n",
      "19 0.001 0.6416\n",
      "0.1 0.002 0.6824\n",
      "0.2 0.002 0.682\n",
      "0.3 0.002 0.6788\n",
      "0.4 0.002 0.6786\n",
      "0.5 0.002 0.6804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 0.002 0.68\n",
      "0.7 0.002 0.6768\n",
      "0.8 0.002 0.6764\n",
      "0.9 0.002 0.675\n",
      "0.01 0.002 0.5932\n",
      "0.02 0.002 0.641\n",
      "0.03 0.002 0.6568\n",
      "0.04 0.002 0.6644\n",
      "0.05 0.002 0.6706\n",
      "0.06 0.002 0.6754\n",
      "0.07 0.002 0.678\n",
      "0.08 0.002 0.68\n",
      "0.09 0.002 0.6816\n",
      "0.001 0.002 0.4176\n",
      "0.002 0.002 0.4386\n",
      "0.003 0.002 0.4714\n",
      "0.004 0.002 0.4942\n",
      "0.005 0.002 0.525\n",
      "0.006 0.002 0.5472\n",
      "0.007 0.002 0.563\n",
      "0.008 0.002 0.5736\n",
      "0.009 0.002 0.5846\n",
      "0.0001 0.002 0.4\n",
      "0.0002 0.002 0.4\n",
      "0.0003 0.002 0.4\n",
      "0.0004 0.002 0.4\n",
      "0.0005 0.002 0.4002\n",
      "0.0006 0.002 0.4056\n",
      "0.0007 0.002 0.4114\n",
      "0.0008 0.002 0.4144\n",
      "0.0009 0.002 0.4166\n",
      "1 0.002 0.6728\n",
      "2 0.002 0.6624\n",
      "3 0.002 0.6552\n",
      "4 0.002 0.652\n",
      "5 0.002 0.6496\n",
      "6 0.002 0.6488\n",
      "7 0.002 0.6476\n",
      "8 0.002 0.6472\n",
      "9 0.002 0.6458\n",
      "10 0.002 0.6456\n",
      "11 0.002 0.6444\n",
      "12 0.002 0.644\n",
      "13 0.002 0.6434\n",
      "14 0.002 0.643\n",
      "15 0.002 0.6432\n",
      "16 0.002 0.6426\n",
      "17 0.002 0.6426\n",
      "18 0.002 0.6424\n",
      "19 0.002 0.6416\n",
      "0.1 0.003 0.6824\n",
      "0.2 0.003 0.682\n",
      "0.3 0.003 0.6788\n",
      "0.4 0.003 0.6786\n",
      "0.5 0.003 0.6804\n",
      "0.6 0.003 0.68\n",
      "0.7 0.003 0.6768\n",
      "0.8 0.003 0.6764\n",
      "0.9 0.003 0.675\n",
      "0.01 0.003 0.5932\n",
      "0.02 0.003 0.641\n",
      "0.03 0.003 0.657\n",
      "0.04 0.003 0.6644\n",
      "0.05 0.003 0.6706\n",
      "0.06 0.003 0.6754\n",
      "0.07 0.003 0.678\n",
      "0.08 0.003 0.68\n",
      "0.09 0.003 0.6816\n",
      "0.001 0.003 0.4176\n",
      "0.002 0.003 0.4386\n",
      "0.003 0.003 0.4714\n",
      "0.004 0.003 0.4942\n",
      "0.005 0.003 0.525\n",
      "0.006 0.003 0.5472\n",
      "0.007 0.003 0.563\n",
      "0.008 0.003 0.5736\n",
      "0.009 0.003 0.5846\n",
      "0.0001 0.003 0.4\n",
      "0.0002 0.003 0.4\n",
      "0.0003 0.003 0.4\n",
      "0.0004 0.003 0.4\n",
      "0.0005 0.003 0.4002\n",
      "0.0006 0.003 0.4056\n",
      "0.0007 0.003 0.4114\n",
      "0.0008 0.003 0.4144\n",
      "0.0009 0.003 0.4166\n",
      "1 0.003 0.6728\n",
      "2 0.003 0.6624\n",
      "3 0.003 0.6552\n",
      "4 0.003 0.652\n",
      "5 0.003 0.6496\n",
      "6 0.003 0.6488\n",
      "7 0.003 0.6476\n",
      "8 0.003 0.6472\n",
      "9 0.003 0.6458\n",
      "10 0.003 0.6456\n",
      "11 0.003 0.6444\n",
      "12 0.003 0.644\n",
      "13 0.003 0.6434\n",
      "14 0.003 0.643\n",
      "15 0.003 0.6432\n",
      "16 0.003 0.6426\n",
      "17 0.003 0.6426\n",
      "18 0.003 0.6424\n",
      "19 0.003 0.6416\n",
      "0.1 0.004 0.6824\n",
      "0.2 0.004 0.682\n",
      "0.3 0.004 0.6788\n",
      "0.4 0.004 0.6786\n",
      "0.5 0.004 0.6804\n",
      "0.6 0.004 0.6798\n",
      "0.7 0.004 0.6768\n",
      "0.8 0.004 0.6764\n",
      "0.9 0.004 0.675\n",
      "0.01 0.004 0.5932\n",
      "0.02 0.004 0.641\n",
      "0.03 0.004 0.657\n",
      "0.04 0.004 0.6644\n",
      "0.05 0.004 0.6706\n",
      "0.06 0.004 0.6754\n",
      "0.07 0.004 0.678\n",
      "0.08 0.004 0.68\n",
      "0.09 0.004 0.6816\n",
      "0.001 0.004 0.4174\n",
      "0.002 0.004 0.4386\n",
      "0.003 0.004 0.4714\n",
      "0.004 0.004 0.4942\n",
      "0.005 0.004 0.525\n",
      "0.006 0.004 0.5472\n",
      "0.007 0.004 0.563\n",
      "0.008 0.004 0.5736\n",
      "0.009 0.004 0.5846\n",
      "0.0001 0.004 0.4\n",
      "0.0002 0.004 0.4\n",
      "0.0003 0.004 0.4\n",
      "0.0004 0.004 0.4\n",
      "0.0005 0.004 0.4002\n",
      "0.0006 0.004 0.4056\n",
      "0.0007 0.004 0.4114\n",
      "0.0008 0.004 0.4144\n",
      "0.0009 0.004 0.4166\n",
      "1 0.004 0.6728\n",
      "2 0.004 0.6624\n",
      "3 0.004 0.6552\n",
      "4 0.004 0.652\n",
      "5 0.004 0.6496\n",
      "6 0.004 0.6488\n",
      "7 0.004 0.6476\n",
      "8 0.004 0.6472\n",
      "9 0.004 0.6458\n",
      "10 0.004 0.6456\n",
      "11 0.004 0.6444\n",
      "12 0.004 0.644\n",
      "13 0.004 0.6434\n",
      "14 0.004 0.643\n",
      "15 0.004 0.6432\n",
      "16 0.004 0.6426\n",
      "17 0.004 0.6426\n",
      "18 0.004 0.6424\n",
      "19 0.004 0.6416\n",
      "0.1 0.005 0.6824\n",
      "0.2 0.005 0.6818\n",
      "0.3 0.005 0.6788\n",
      "0.4 0.005 0.6786\n",
      "0.5 0.005 0.6802\n",
      "0.6 0.005 0.68\n",
      "0.7 0.005 0.6768\n",
      "0.8 0.005 0.6764\n",
      "0.9 0.005 0.675\n",
      "0.01 0.005 0.5932\n",
      "0.02 0.005 0.641\n",
      "0.03 0.005 0.6568\n",
      "0.04 0.005 0.6644\n",
      "0.05 0.005 0.6706\n",
      "0.06 0.005 0.6752\n",
      "0.07 0.005 0.678\n",
      "0.08 0.005 0.68\n",
      "0.09 0.005 0.6816\n",
      "0.001 0.005 0.4176\n",
      "0.002 0.005 0.4386\n",
      "0.003 0.005 0.4714\n",
      "0.004 0.005 0.4942\n",
      "0.005 0.005 0.525\n",
      "0.006 0.005 0.5472\n",
      "0.007 0.005 0.563\n",
      "0.008 0.005 0.5736\n",
      "0.009 0.005 0.5846\n",
      "0.0001 0.005 0.4\n",
      "0.0002 0.005 0.4\n",
      "0.0003 0.005 0.4\n",
      "0.0004 0.005 0.4\n",
      "0.0005 0.005 0.4002\n",
      "0.0006 0.005 0.4056\n",
      "0.0007 0.005 0.4114\n",
      "0.0008 0.005 0.4144\n",
      "0.0009 0.005 0.4166\n",
      "1 0.005 0.6728\n",
      "2 0.005 0.6624\n",
      "3 0.005 0.6552\n",
      "4 0.005 0.652\n",
      "5 0.005 0.6496\n",
      "6 0.005 0.6488\n",
      "7 0.005 0.6476\n",
      "8 0.005 0.6472\n",
      "9 0.005 0.646\n",
      "10 0.005 0.6456\n",
      "11 0.005 0.6444\n",
      "12 0.005 0.644\n",
      "13 0.005 0.6434\n",
      "14 0.005 0.643\n",
      "15 0.005 0.6432\n",
      "16 0.005 0.6426\n",
      "17 0.005 0.6426\n",
      "18 0.005 0.6424\n",
      "19 0.005 0.6416\n",
      "0.1 0.006 0.6824\n",
      "0.2 0.006 0.6818\n",
      "0.3 0.006 0.6786\n",
      "0.4 0.006 0.6786\n",
      "0.5 0.006 0.6804\n",
      "0.6 0.006 0.68\n",
      "0.7 0.006 0.6768\n",
      "0.8 0.006 0.6764\n",
      "0.9 0.006 0.675\n",
      "0.01 0.006 0.5932\n",
      "0.02 0.006 0.641\n",
      "0.03 0.006 0.6568\n",
      "0.04 0.006 0.6644\n",
      "0.05 0.006 0.6706\n",
      "0.06 0.006 0.6752\n",
      "0.07 0.006 0.678\n",
      "0.08 0.006 0.68\n",
      "0.09 0.006 0.6816\n",
      "0.001 0.006 0.4174\n",
      "0.002 0.006 0.4386\n",
      "0.003 0.006 0.4714\n",
      "0.004 0.006 0.4942\n",
      "0.005 0.006 0.525\n",
      "0.006 0.006 0.5472\n",
      "0.007 0.006 0.563\n",
      "0.008 0.006 0.5736\n",
      "0.009 0.006 0.5846\n",
      "0.0001 0.006 0.4\n",
      "0.0002 0.006 0.4\n",
      "0.0003 0.006 0.4\n",
      "0.0004 0.006 0.4\n",
      "0.0005 0.006 0.4002\n",
      "0.0006 0.006 0.4056\n",
      "0.0007 0.006 0.4114\n",
      "0.0008 0.006 0.4144\n",
      "0.0009 0.006 0.4166\n",
      "1 0.006 0.6728\n",
      "2 0.006 0.6624\n",
      "3 0.006 0.6552\n",
      "4 0.006 0.652\n",
      "5 0.006 0.6494\n",
      "6 0.006 0.6488\n",
      "7 0.006 0.6476\n",
      "8 0.006 0.6472\n",
      "9 0.006 0.646\n",
      "10 0.006 0.6456\n",
      "11 0.006 0.6444\n",
      "12 0.006 0.644\n",
      "13 0.006 0.6434\n",
      "14 0.006 0.643\n",
      "15 0.006 0.6432\n",
      "16 0.006 0.6428\n",
      "17 0.006 0.6426\n",
      "18 0.006 0.6422\n",
      "19 0.006 0.6416\n",
      "0.1 0.007 0.6824\n",
      "0.2 0.007 0.682\n",
      "0.3 0.007 0.6788\n",
      "0.4 0.007 0.6786\n",
      "0.5 0.007 0.6804\n",
      "0.6 0.007 0.68\n",
      "0.7 0.007 0.6768\n",
      "0.8 0.007 0.6764\n",
      "0.9 0.007 0.675\n",
      "0.01 0.007 0.5932\n",
      "0.02 0.007 0.641\n",
      "0.03 0.007 0.6568\n",
      "0.04 0.007 0.6644\n",
      "0.05 0.007 0.6706\n",
      "0.06 0.007 0.6754\n",
      "0.07 0.007 0.678\n",
      "0.08 0.007 0.68\n",
      "0.09 0.007 0.6816\n",
      "0.001 0.007 0.4176\n",
      "0.002 0.007 0.4386\n",
      "0.003 0.007 0.4714\n",
      "0.004 0.007 0.4942\n",
      "0.005 0.007 0.525\n",
      "0.006 0.007 0.5472\n",
      "0.007 0.007 0.563\n",
      "0.008 0.007 0.5736\n",
      "0.009 0.007 0.5846\n",
      "0.0001 0.007 0.4\n",
      "0.0002 0.007 0.4\n",
      "0.0003 0.007 0.4\n",
      "0.0004 0.007 0.4\n",
      "0.0005 0.007 0.4002\n",
      "0.0006 0.007 0.4056\n",
      "0.0007 0.007 0.4114\n",
      "0.0008 0.007 0.4144\n",
      "0.0009 0.007 0.4166\n",
      "1 0.007 0.6728\n",
      "2 0.007 0.6624\n",
      "3 0.007 0.6552\n",
      "4 0.007 0.652\n",
      "5 0.007 0.6496\n",
      "6 0.007 0.6488\n",
      "7 0.007 0.6476\n",
      "8 0.007 0.647\n",
      "9 0.007 0.646\n",
      "10 0.007 0.6456\n",
      "11 0.007 0.6444\n",
      "12 0.007 0.644\n",
      "13 0.007 0.6434\n",
      "14 0.007 0.643\n",
      "15 0.007 0.6432\n",
      "16 0.007 0.6426\n",
      "17 0.007 0.6426\n",
      "18 0.007 0.6422\n",
      "19 0.007 0.6416\n",
      "0.1 0.008 0.6824\n",
      "0.2 0.008 0.6818\n",
      "0.3 0.008 0.6788\n",
      "0.4 0.008 0.6786\n",
      "0.5 0.008 0.6804\n",
      "0.6 0.008 0.68\n",
      "0.7 0.008 0.6768\n",
      "0.8 0.008 0.6764\n",
      "0.9 0.008 0.6748\n",
      "0.01 0.008 0.5932\n",
      "0.02 0.008 0.641\n",
      "0.03 0.008 0.6568\n",
      "0.04 0.008 0.6644\n",
      "0.05 0.008 0.6706\n",
      "0.06 0.008 0.6754\n",
      "0.07 0.008 0.678\n",
      "0.08 0.008 0.6798\n",
      "0.09 0.008 0.6816\n",
      "0.001 0.008 0.4174\n",
      "0.002 0.008 0.4386\n",
      "0.003 0.008 0.4714\n",
      "0.004 0.008 0.4942\n",
      "0.005 0.008 0.5248\n",
      "0.006 0.008 0.5472\n",
      "0.007 0.008 0.563\n",
      "0.008 0.008 0.5736\n",
      "0.009 0.008 0.5846\n",
      "0.0001 0.008 0.4\n",
      "0.0002 0.008 0.4\n",
      "0.0003 0.008 0.4\n",
      "0.0004 0.008 0.4\n",
      "0.0005 0.008 0.4002\n",
      "0.0006 0.008 0.4056\n",
      "0.0007 0.008 0.4114\n",
      "0.0008 0.008 0.4144\n",
      "0.0009 0.008 0.4166\n",
      "1 0.008 0.6728\n",
      "2 0.008 0.6624\n",
      "3 0.008 0.6552\n",
      "4 0.008 0.652\n",
      "5 0.008 0.6494\n",
      "6 0.008 0.649\n",
      "7 0.008 0.6476\n",
      "8 0.008 0.647\n",
      "9 0.008 0.646\n",
      "10 0.008 0.6456\n",
      "11 0.008 0.6444\n",
      "12 0.008 0.644\n",
      "13 0.008 0.6434\n",
      "14 0.008 0.643\n",
      "15 0.008 0.6432\n",
      "16 0.008 0.6428\n",
      "17 0.008 0.6426\n",
      "18 0.008 0.6424\n",
      "19 0.008 0.6416\n",
      "0.1 0.009 0.6824\n",
      "0.2 0.009 0.6818\n",
      "0.3 0.009 0.6788\n",
      "0.4 0.009 0.6788\n",
      "0.5 0.009 0.6804\n",
      "0.6 0.009 0.6796\n",
      "0.7 0.009 0.6762\n",
      "0.8 0.009 0.6764\n",
      "0.9 0.009 0.675\n",
      "0.01 0.009 0.5932\n",
      "0.02 0.009 0.641\n",
      "0.03 0.009 0.657\n",
      "0.04 0.009 0.6644\n",
      "0.05 0.009 0.6706\n",
      "0.06 0.009 0.6754\n",
      "0.07 0.009 0.678\n",
      "0.08 0.009 0.68\n",
      "0.09 0.009 0.6816\n",
      "0.001 0.009 0.4174\n",
      "0.002 0.009 0.4386\n",
      "0.003 0.009 0.4714\n",
      "0.004 0.009 0.4942\n",
      "0.005 0.009 0.5248\n",
      "0.006 0.009 0.5472\n",
      "0.007 0.009 0.563\n",
      "0.008 0.009 0.5736\n",
      "0.009 0.009 0.5846\n",
      "0.0001 0.009 0.4\n",
      "0.0002 0.009 0.4\n",
      "0.0003 0.009 0.4\n",
      "0.0004 0.009 0.4\n",
      "0.0005 0.009 0.4002\n",
      "0.0006 0.009 0.4056\n",
      "0.0007 0.009 0.4114\n",
      "0.0008 0.009 0.4144\n",
      "0.0009 0.009 0.4166\n",
      "1 0.009 0.673\n",
      "2 0.009 0.6624\n",
      "3 0.009 0.655\n",
      "4 0.009 0.652\n",
      "5 0.009 0.6494\n",
      "6 0.009 0.6488\n",
      "7 0.009 0.6476\n",
      "8 0.009 0.647\n",
      "9 0.009 0.646\n",
      "10 0.009 0.6456\n",
      "11 0.009 0.6444\n",
      "12 0.009 0.644\n",
      "13 0.009 0.6434\n",
      "14 0.009 0.643\n",
      "15 0.009 0.6432\n",
      "16 0.009 0.6426\n",
      "17 0.009 0.6426\n",
      "18 0.009 0.6422\n",
      "19 0.009 0.6416\n",
      "0.1 0.0001 0.6824\n",
      "0.2 0.0001 0.682\n",
      "0.3 0.0001 0.6788\n",
      "0.4 0.0001 0.6786\n",
      "0.5 0.0001 0.6804\n",
      "0.6 0.0001 0.68\n",
      "0.7 0.0001 0.6768\n",
      "0.8 0.0001 0.6764\n",
      "0.9 0.0001 0.675\n",
      "0.01 0.0001 0.5932\n",
      "0.02 0.0001 0.641\n",
      "0.03 0.0001 0.6568\n",
      "0.04 0.0001 0.6644\n",
      "0.05 0.0001 0.6706\n",
      "0.06 0.0001 0.6754\n",
      "0.07 0.0001 0.678\n",
      "0.08 0.0001 0.68\n",
      "0.09 0.0001 0.6816\n",
      "0.001 0.0001 0.4174\n",
      "0.002 0.0001 0.4386\n",
      "0.003 0.0001 0.4714\n",
      "0.004 0.0001 0.4942\n",
      "0.005 0.0001 0.525\n",
      "0.006 0.0001 0.5472\n",
      "0.007 0.0001 0.563\n",
      "0.008 0.0001 0.5736\n",
      "0.009 0.0001 0.5846\n",
      "0.0001 0.0001 0.4\n",
      "0.0002 0.0001 0.4\n",
      "0.0003 0.0001 0.4\n",
      "0.0004 0.0001 0.4\n",
      "0.0005 0.0001 0.4002\n",
      "0.0006 0.0001 0.4056\n",
      "0.0007 0.0001 0.4114\n",
      "0.0008 0.0001 0.4144\n",
      "0.0009 0.0001 0.4166\n",
      "1 0.0001 0.6728\n",
      "2 0.0001 0.6624\n",
      "3 0.0001 0.6552\n",
      "4 0.0001 0.652\n",
      "5 0.0001 0.6496\n",
      "6 0.0001 0.6488\n",
      "7 0.0001 0.6476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.0001 0.6472\n",
      "9 0.0001 0.6458\n",
      "10 0.0001 0.6456\n",
      "11 0.0001 0.6444\n",
      "12 0.0001 0.644\n",
      "13 0.0001 0.6434\n",
      "14 0.0001 0.643\n",
      "15 0.0001 0.6432\n",
      "16 0.0001 0.6426\n",
      "17 0.0001 0.6426\n",
      "18 0.0001 0.6424\n",
      "19 0.0001 0.6416\n",
      "0.1 0.0002 0.6824\n",
      "0.2 0.0002 0.682\n",
      "0.3 0.0002 0.6788\n",
      "0.4 0.0002 0.6786\n",
      "0.5 0.0002 0.6804\n",
      "0.6 0.0002 0.68\n",
      "0.7 0.0002 0.6768\n",
      "0.8 0.0002 0.6764\n",
      "0.9 0.0002 0.675\n",
      "0.01 0.0002 0.5932\n",
      "0.02 0.0002 0.641\n",
      "0.03 0.0002 0.6568\n",
      "0.04 0.0002 0.6644\n",
      "0.05 0.0002 0.6706\n",
      "0.06 0.0002 0.6754\n",
      "0.07 0.0002 0.678\n",
      "0.08 0.0002 0.68\n",
      "0.09 0.0002 0.6816\n",
      "0.001 0.0002 0.4174\n",
      "0.002 0.0002 0.4386\n",
      "0.003 0.0002 0.4714\n",
      "0.004 0.0002 0.4942\n",
      "0.005 0.0002 0.525\n",
      "0.006 0.0002 0.5472\n",
      "0.007 0.0002 0.563\n",
      "0.008 0.0002 0.5736\n",
      "0.009 0.0002 0.5846\n",
      "0.0001 0.0002 0.4\n",
      "0.0002 0.0002 0.4\n",
      "0.0003 0.0002 0.4\n",
      "0.0004 0.0002 0.4\n",
      "0.0005 0.0002 0.4002\n",
      "0.0006 0.0002 0.4056\n",
      "0.0007 0.0002 0.4114\n",
      "0.0008 0.0002 0.4144\n",
      "0.0009 0.0002 0.4166\n",
      "1 0.0002 0.6728\n",
      "2 0.0002 0.6624\n",
      "3 0.0002 0.6552\n",
      "4 0.0002 0.652\n",
      "5 0.0002 0.6496\n",
      "6 0.0002 0.6488\n",
      "7 0.0002 0.6476\n",
      "8 0.0002 0.6472\n",
      "9 0.0002 0.6458\n",
      "10 0.0002 0.6456\n",
      "11 0.0002 0.6444\n",
      "12 0.0002 0.644\n",
      "13 0.0002 0.6434\n",
      "14 0.0002 0.643\n",
      "15 0.0002 0.6432\n",
      "16 0.0002 0.6426\n",
      "17 0.0002 0.6426\n",
      "18 0.0002 0.6424\n",
      "19 0.0002 0.6416\n",
      "0.1 0.0003 0.6824\n",
      "0.2 0.0003 0.682\n",
      "0.3 0.0003 0.6788\n",
      "0.4 0.0003 0.6786\n",
      "0.5 0.0003 0.6804\n",
      "0.6 0.0003 0.68\n",
      "0.7 0.0003 0.6768\n",
      "0.8 0.0003 0.6764\n",
      "0.9 0.0003 0.675\n",
      "0.01 0.0003 0.5932\n",
      "0.02 0.0003 0.641\n",
      "0.03 0.0003 0.6568\n",
      "0.04 0.0003 0.6644\n",
      "0.05 0.0003 0.6706\n",
      "0.06 0.0003 0.6754\n",
      "0.07 0.0003 0.678\n",
      "0.08 0.0003 0.68\n",
      "0.09 0.0003 0.6816\n",
      "0.001 0.0003 0.4174\n",
      "0.002 0.0003 0.4386\n",
      "0.003 0.0003 0.4714\n",
      "0.004 0.0003 0.4942\n",
      "0.005 0.0003 0.525\n",
      "0.006 0.0003 0.5472\n",
      "0.007 0.0003 0.563\n",
      "0.008 0.0003 0.5736\n",
      "0.009 0.0003 0.5846\n",
      "0.0001 0.0003 0.4\n",
      "0.0002 0.0003 0.4\n",
      "0.0003 0.0003 0.4\n",
      "0.0004 0.0003 0.4\n",
      "0.0005 0.0003 0.4002\n",
      "0.0006 0.0003 0.4056\n",
      "0.0007 0.0003 0.4114\n",
      "0.0008 0.0003 0.4144\n",
      "0.0009 0.0003 0.4166\n",
      "1 0.0003 0.6728\n",
      "2 0.0003 0.6624\n",
      "3 0.0003 0.6552\n",
      "4 0.0003 0.652\n",
      "5 0.0003 0.6496\n",
      "6 0.0003 0.6488\n",
      "7 0.0003 0.6476\n",
      "8 0.0003 0.6472\n",
      "9 0.0003 0.6458\n",
      "10 0.0003 0.6456\n",
      "11 0.0003 0.6444\n",
      "12 0.0003 0.644\n",
      "13 0.0003 0.6434\n",
      "14 0.0003 0.643\n",
      "15 0.0003 0.6432\n",
      "16 0.0003 0.6426\n",
      "17 0.0003 0.6426\n",
      "18 0.0003 0.6424\n",
      "19 0.0003 0.6416\n",
      "0.1 0.0004 0.6824\n",
      "0.2 0.0004 0.682\n",
      "0.3 0.0004 0.6788\n",
      "0.4 0.0004 0.6786\n",
      "0.5 0.0004 0.6804\n",
      "0.6 0.0004 0.68\n",
      "0.7 0.0004 0.6768\n",
      "0.8 0.0004 0.6764\n",
      "0.9 0.0004 0.675\n",
      "0.01 0.0004 0.5932\n",
      "0.02 0.0004 0.641\n",
      "0.03 0.0004 0.6568\n",
      "0.04 0.0004 0.6644\n",
      "0.05 0.0004 0.6706\n",
      "0.06 0.0004 0.6754\n",
      "0.07 0.0004 0.678\n",
      "0.08 0.0004 0.68\n",
      "0.09 0.0004 0.6816\n",
      "0.001 0.0004 0.4174\n",
      "0.002 0.0004 0.4386\n",
      "0.003 0.0004 0.4714\n",
      "0.004 0.0004 0.4942\n",
      "0.005 0.0004 0.525\n",
      "0.006 0.0004 0.5472\n",
      "0.007 0.0004 0.563\n",
      "0.008 0.0004 0.5736\n",
      "0.009 0.0004 0.5846\n",
      "0.0001 0.0004 0.4\n",
      "0.0002 0.0004 0.4\n",
      "0.0003 0.0004 0.4\n",
      "0.0004 0.0004 0.4\n",
      "0.0005 0.0004 0.4002\n",
      "0.0006 0.0004 0.4056\n",
      "0.0007 0.0004 0.4114\n",
      "0.0008 0.0004 0.4144\n",
      "0.0009 0.0004 0.4166\n",
      "1 0.0004 0.6728\n",
      "2 0.0004 0.6624\n",
      "3 0.0004 0.6552\n",
      "4 0.0004 0.652\n",
      "5 0.0004 0.6496\n",
      "6 0.0004 0.6488\n",
      "7 0.0004 0.6476\n",
      "8 0.0004 0.6472\n",
      "9 0.0004 0.6458\n",
      "10 0.0004 0.6456\n",
      "11 0.0004 0.6444\n",
      "12 0.0004 0.644\n",
      "13 0.0004 0.6434\n",
      "14 0.0004 0.643\n",
      "15 0.0004 0.6432\n",
      "16 0.0004 0.6426\n",
      "17 0.0004 0.6426\n",
      "18 0.0004 0.6424\n",
      "19 0.0004 0.6416\n",
      "0.1 0.0005 0.6824\n",
      "0.2 0.0005 0.682\n",
      "0.3 0.0005 0.6788\n",
      "0.4 0.0005 0.6786\n",
      "0.5 0.0005 0.6804\n",
      "0.6 0.0005 0.68\n",
      "0.7 0.0005 0.6768\n",
      "0.8 0.0005 0.6764\n",
      "0.9 0.0005 0.675\n",
      "0.01 0.0005 0.5932\n",
      "0.02 0.0005 0.641\n",
      "0.03 0.0005 0.6568\n",
      "0.04 0.0005 0.6644\n",
      "0.05 0.0005 0.6706\n",
      "0.06 0.0005 0.6754\n",
      "0.07 0.0005 0.678\n",
      "0.08 0.0005 0.68\n",
      "0.09 0.0005 0.6816\n",
      "0.001 0.0005 0.4174\n",
      "0.002 0.0005 0.4386\n",
      "0.003 0.0005 0.4714\n",
      "0.004 0.0005 0.4942\n",
      "0.005 0.0005 0.525\n",
      "0.006 0.0005 0.5472\n",
      "0.007 0.0005 0.563\n",
      "0.008 0.0005 0.5736\n",
      "0.009 0.0005 0.5846\n",
      "0.0001 0.0005 0.4\n",
      "0.0002 0.0005 0.4\n",
      "0.0003 0.0005 0.4\n",
      "0.0004 0.0005 0.4\n",
      "0.0005 0.0005 0.4002\n",
      "0.0006 0.0005 0.4056\n",
      "0.0007 0.0005 0.4114\n",
      "0.0008 0.0005 0.4144\n",
      "0.0009 0.0005 0.4166\n",
      "1 0.0005 0.6728\n",
      "2 0.0005 0.6624\n",
      "3 0.0005 0.6552\n",
      "4 0.0005 0.652\n",
      "5 0.0005 0.6496\n",
      "6 0.0005 0.6488\n",
      "7 0.0005 0.6476\n",
      "8 0.0005 0.6472\n",
      "9 0.0005 0.6458\n",
      "10 0.0005 0.6456\n",
      "11 0.0005 0.6444\n",
      "12 0.0005 0.644\n",
      "13 0.0005 0.6434\n",
      "14 0.0005 0.643\n",
      "15 0.0005 0.6432\n",
      "16 0.0005 0.6426\n",
      "17 0.0005 0.6426\n",
      "18 0.0005 0.6424\n",
      "19 0.0005 0.6416\n",
      "0.1 0.0006 0.6824\n",
      "0.2 0.0006 0.682\n",
      "0.3 0.0006 0.6788\n",
      "0.4 0.0006 0.6786\n",
      "0.5 0.0006 0.6804\n",
      "0.6 0.0006 0.68\n",
      "0.7 0.0006 0.6768\n",
      "0.8 0.0006 0.6764\n",
      "0.9 0.0006 0.675\n",
      "0.01 0.0006 0.5932\n",
      "0.02 0.0006 0.641\n",
      "0.03 0.0006 0.6568\n",
      "0.04 0.0006 0.6644\n",
      "0.05 0.0006 0.6706\n",
      "0.06 0.0006 0.6754\n",
      "0.07 0.0006 0.678\n",
      "0.08 0.0006 0.68\n",
      "0.09 0.0006 0.6816\n",
      "0.001 0.0006 0.4174\n",
      "0.002 0.0006 0.4386\n",
      "0.003 0.0006 0.4714\n",
      "0.004 0.0006 0.4942\n",
      "0.005 0.0006 0.525\n",
      "0.006 0.0006 0.5472\n",
      "0.007 0.0006 0.563\n",
      "0.008 0.0006 0.5736\n",
      "0.009 0.0006 0.5846\n",
      "0.0001 0.0006 0.4\n",
      "0.0002 0.0006 0.4\n",
      "0.0003 0.0006 0.4\n",
      "0.0004 0.0006 0.4\n",
      "0.0005 0.0006 0.4002\n",
      "0.0006 0.0006 0.4056\n",
      "0.0007 0.0006 0.4114\n",
      "0.0008 0.0006 0.4144\n",
      "0.0009 0.0006 0.4166\n",
      "1 0.0006 0.6728\n",
      "2 0.0006 0.6624\n",
      "3 0.0006 0.6552\n",
      "4 0.0006 0.652\n",
      "5 0.0006 0.6496\n",
      "6 0.0006 0.6488\n",
      "7 0.0006 0.6476\n",
      "8 0.0006 0.6472\n",
      "9 0.0006 0.6458\n",
      "10 0.0006 0.6456\n",
      "11 0.0006 0.6444\n",
      "12 0.0006 0.644\n",
      "13 0.0006 0.6434\n",
      "14 0.0006 0.643\n",
      "15 0.0006 0.6432\n",
      "16 0.0006 0.6426\n",
      "17 0.0006 0.6426\n",
      "18 0.0006 0.6424\n",
      "19 0.0006 0.6416\n",
      "0.1 0.0007 0.6824\n",
      "0.2 0.0007 0.682\n",
      "0.3 0.0007 0.6788\n",
      "0.4 0.0007 0.6786\n",
      "0.5 0.0007 0.6804\n",
      "0.6 0.0007 0.68\n",
      "0.7 0.0007 0.6768\n",
      "0.8 0.0007 0.6764\n",
      "0.9 0.0007 0.675\n",
      "0.01 0.0007 0.5932\n",
      "0.02 0.0007 0.641\n",
      "0.03 0.0007 0.6568\n",
      "0.04 0.0007 0.6644\n",
      "0.05 0.0007 0.6706\n",
      "0.06 0.0007 0.6754\n",
      "0.07 0.0007 0.678\n",
      "0.08 0.0007 0.68\n",
      "0.09 0.0007 0.6816\n",
      "0.001 0.0007 0.4174\n",
      "0.002 0.0007 0.4386\n",
      "0.003 0.0007 0.4714\n",
      "0.004 0.0007 0.4942\n",
      "0.005 0.0007 0.525\n",
      "0.006 0.0007 0.5472\n",
      "0.007 0.0007 0.563\n",
      "0.008 0.0007 0.5736\n",
      "0.009 0.0007 0.5846\n",
      "0.0001 0.0007 0.4\n",
      "0.0002 0.0007 0.4\n",
      "0.0003 0.0007 0.4\n",
      "0.0004 0.0007 0.4\n",
      "0.0005 0.0007 0.4002\n",
      "0.0006 0.0007 0.4056\n",
      "0.0007 0.0007 0.4114\n",
      "0.0008 0.0007 0.4144\n",
      "0.0009 0.0007 0.4166\n",
      "1 0.0007 0.6728\n",
      "2 0.0007 0.6624\n",
      "3 0.0007 0.6552\n",
      "4 0.0007 0.652\n",
      "5 0.0007 0.6496\n",
      "6 0.0007 0.6488\n",
      "7 0.0007 0.6476\n",
      "8 0.0007 0.6472\n",
      "9 0.0007 0.6458\n",
      "10 0.0007 0.6456\n",
      "11 0.0007 0.6444\n",
      "12 0.0007 0.644\n",
      "13 0.0007 0.6434\n",
      "14 0.0007 0.643\n",
      "15 0.0007 0.6432\n",
      "16 0.0007 0.6426\n",
      "17 0.0007 0.6426\n",
      "18 0.0007 0.6424\n",
      "19 0.0007 0.6416\n",
      "0.1 0.0008 0.6824\n",
      "0.2 0.0008 0.682\n",
      "0.3 0.0008 0.6788\n",
      "0.4 0.0008 0.6786\n",
      "0.5 0.0008 0.6804\n",
      "0.6 0.0008 0.68\n",
      "0.7 0.0008 0.6768\n",
      "0.8 0.0008 0.6764\n",
      "0.9 0.0008 0.675\n",
      "0.01 0.0008 0.5932\n",
      "0.02 0.0008 0.641\n",
      "0.03 0.0008 0.6568\n",
      "0.04 0.0008 0.6644\n",
      "0.05 0.0008 0.6706\n",
      "0.06 0.0008 0.6754\n",
      "0.07 0.0008 0.678\n",
      "0.08 0.0008 0.68\n",
      "0.09 0.0008 0.6816\n",
      "0.001 0.0008 0.4174\n",
      "0.002 0.0008 0.4386\n",
      "0.003 0.0008 0.4714\n",
      "0.004 0.0008 0.4942\n",
      "0.005 0.0008 0.525\n",
      "0.006 0.0008 0.5472\n",
      "0.007 0.0008 0.563\n",
      "0.008 0.0008 0.5736\n",
      "0.009 0.0008 0.5846\n",
      "0.0001 0.0008 0.4\n",
      "0.0002 0.0008 0.4\n",
      "0.0003 0.0008 0.4\n",
      "0.0004 0.0008 0.4\n",
      "0.0005 0.0008 0.4002\n",
      "0.0006 0.0008 0.4056\n",
      "0.0007 0.0008 0.4114\n",
      "0.0008 0.0008 0.4144\n",
      "0.0009 0.0008 0.4166\n",
      "1 0.0008 0.6728\n",
      "2 0.0008 0.6624\n",
      "3 0.0008 0.6552\n",
      "4 0.0008 0.652\n",
      "5 0.0008 0.6496\n",
      "6 0.0008 0.6488\n",
      "7 0.0008 0.6476\n",
      "8 0.0008 0.6472\n",
      "9 0.0008 0.6458\n",
      "10 0.0008 0.6456\n",
      "11 0.0008 0.6444\n",
      "12 0.0008 0.644\n",
      "13 0.0008 0.6434\n",
      "14 0.0008 0.643\n",
      "15 0.0008 0.6432\n",
      "16 0.0008 0.6426\n",
      "17 0.0008 0.6426\n",
      "18 0.0008 0.6424\n",
      "19 0.0008 0.6416\n",
      "0.1 0.0009 0.6824\n",
      "0.2 0.0009 0.682\n",
      "0.3 0.0009 0.6788\n",
      "0.4 0.0009 0.6786\n",
      "0.5 0.0009 0.6804\n",
      "0.6 0.0009 0.68\n",
      "0.7 0.0009 0.6768\n",
      "0.8 0.0009 0.6764\n",
      "0.9 0.0009 0.675\n",
      "0.01 0.0009 0.5932\n",
      "0.02 0.0009 0.641\n",
      "0.03 0.0009 0.6568\n",
      "0.04 0.0009 0.6644\n",
      "0.05 0.0009 0.6706\n",
      "0.06 0.0009 0.6754\n",
      "0.07 0.0009 0.678\n",
      "0.08 0.0009 0.68\n",
      "0.09 0.0009 0.6816\n",
      "0.001 0.0009 0.4174\n",
      "0.002 0.0009 0.4386\n",
      "0.003 0.0009 0.4714\n",
      "0.004 0.0009 0.4942\n",
      "0.005 0.0009 0.525\n",
      "0.006 0.0009 0.5472\n",
      "0.007 0.0009 0.563\n",
      "0.008 0.0009 0.5736\n",
      "0.009 0.0009 0.5846\n",
      "0.0001 0.0009 0.4\n",
      "0.0002 0.0009 0.4\n",
      "0.0003 0.0009 0.4\n",
      "0.0004 0.0009 0.4\n",
      "0.0005 0.0009 0.4002\n",
      "0.0006 0.0009 0.4056\n",
      "0.0007 0.0009 0.4114\n",
      "0.0008 0.0009 0.4144\n",
      "0.0009 0.0009 0.4166\n",
      "1 0.0009 0.6728\n",
      "2 0.0009 0.6624\n",
      "3 0.0009 0.6552\n",
      "4 0.0009 0.652\n",
      "5 0.0009 0.6496\n",
      "6 0.0009 0.6488\n",
      "7 0.0009 0.6476\n",
      "8 0.0009 0.6472\n",
      "9 0.0009 0.6458\n",
      "10 0.0009 0.6456\n",
      "11 0.0009 0.6444\n",
      "12 0.0009 0.644\n",
      "13 0.0009 0.6434\n",
      "14 0.0009 0.643\n",
      "15 0.0009 0.6432\n",
      "16 0.0009 0.6426\n",
      "17 0.0009 0.6426\n",
      "18 0.0009 0.6424\n",
      "19 0.0009 0.6416\n",
      "0.1 1 0.6772\n",
      "0.2 1 0.679\n",
      "0.3 1 0.6804\n",
      "0.4 1 0.6772\n",
      "0.5 1 0.6784\n",
      "0.6 1 0.6726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7 1 0.6786\n",
      "0.8 1 0.6728\n",
      "0.9 1 0.6762\n",
      "0.01 1 0.5926\n",
      "0.02 1 0.642\n",
      "0.03 1 0.654\n",
      "0.04 1 0.6628\n",
      "0.05 1 0.6742\n",
      "0.06 1 0.6778\n",
      "0.07 1 0.675\n",
      "0.08 1 0.674\n",
      "0.09 1 0.682\n",
      "0.001 1 0.4168\n",
      "0.002 1 0.4322\n",
      "0.003 1 0.4712\n",
      "0.004 1 0.492\n",
      "0.005 1 0.5262\n",
      "0.006 1 0.5474\n",
      "0.007 1 0.5484\n",
      "0.008 1 0.5742\n",
      "0.009 1 0.585\n",
      "0.0001 1 0.4\n",
      "0.0002 1 0.4\n",
      "0.0003 1 0.4\n",
      "0.0004 1 0.4006\n",
      "0.0005 1 0.4004\n",
      "0.0006 1 0.4024\n",
      "0.0007 1 0.4166\n",
      "0.0008 1 0.4206\n",
      "0.0009 1 0.419\n",
      "1 1 0.6768\n",
      "2 1 0.6638\n",
      "3 1 0.656\n",
      "4 1 0.6542\n",
      "5 1 0.653\n",
      "6 1 0.649\n",
      "7 1 0.6492\n",
      "8 1 0.6512\n",
      "9 1 0.6458\n",
      "10 1 0.6468\n",
      "11 1 0.6454\n",
      "12 1 0.6472\n",
      "13 1 0.6432\n",
      "14 1 0.6434\n",
      "15 1 0.6432\n",
      "16 1 0.6466\n",
      "17 1 0.6476\n",
      "18 1 0.6436\n",
      "19 1 0.645\n",
      "0.1 2 0.68\n",
      "0.2 2 0.6796\n",
      "0.3 2 0.6778\n",
      "0.4 2 0.6802\n",
      "0.5 2 0.6778\n",
      "0.6 2 0.6788\n",
      "0.7 2 0.671\n",
      "0.8 2 0.6746\n",
      "0.9 2 0.6756\n",
      "0.01 2 0.5854\n",
      "0.02 2 0.6306\n",
      "0.03 2 0.6588\n",
      "0.04 2 0.6642\n",
      "0.05 2 0.6696\n",
      "0.06 2 0.6748\n",
      "0.07 2 0.6722\n",
      "0.08 2 0.6782\n",
      "0.09 2 0.6804\n",
      "0.001 2 0.4186\n",
      "0.002 2 0.4602\n",
      "0.003 2 0.5436\n",
      "0.004 2 0.4954\n",
      "0.005 2 0.566\n",
      "0.006 2 0.5132\n",
      "0.007 2 0.5858\n",
      "0.008 2 0.5932\n",
      "0.009 2 0.5486\n",
      "0.0001 2 0.4\n",
      "0.0002 2 0.4\n",
      "0.0003 2 0.4\n",
      "0.0004 2 0.4\n",
      "0.0005 2 0.4\n",
      "0.0006 2 0.4054\n",
      "0.0007 2 0.415\n",
      "0.0008 2 0.4166\n",
      "0.0009 2 0.4152\n",
      "1 2 0.6714\n",
      "2 2 0.6596\n",
      "3 2 0.6562\n",
      "4 2 0.6554\n",
      "5 2 0.6572\n",
      "6 2 0.6476\n",
      "7 2 0.6552\n",
      "8 2 0.6514\n",
      "9 2 0.654\n",
      "10 2 0.6504\n",
      "11 2 0.6498\n",
      "12 2 0.6404\n",
      "13 2 0.645\n",
      "14 2 0.6478\n",
      "15 2 0.649\n",
      "16 2 0.6454\n",
      "17 2 0.646\n",
      "18 2 0.6418\n",
      "19 2 0.649\n",
      "0.1 3 0.6254\n",
      "0.2 3 0.5934\n",
      "0.3 3 0.6622\n",
      "0.4 3 0.6574\n",
      "0.5 3 0.6782\n",
      "0.6 3 0.6716\n",
      "0.7 3 0.6658\n",
      "0.8 3 0.6756\n",
      "0.9 3 0.6758\n",
      "0.01 3 0.6172\n",
      "0.02 3 0.6178\n",
      "0.03 3 0.6564\n",
      "0.04 3 0.6392\n",
      "0.05 3 0.5936\n",
      "0.06 3 0.6166\n",
      "0.07 3 0.6594\n",
      "0.08 3 0.672\n",
      "0.09 3 0.6644\n",
      "0.001 3 0.4318\n",
      "0.002 3 0.4444\n",
      "0.003 3 0.4536\n",
      "0.004 3 0.5802\n",
      "0.005 3 0.592\n",
      "0.006 3 0.5168\n",
      "0.007 3 0.527\n",
      "0.008 3 0.5686\n",
      "0.009 3 0.579\n",
      "0.0001 3 0.4\n",
      "0.0002 3 0.4\n",
      "0.0003 3 0.4\n",
      "0.0004 3 0.4\n",
      "0.0005 3 0.4086\n",
      "0.0006 3 0.4\n",
      "0.0007 3 0.4112\n",
      "0.0008 3 0.4146\n",
      "0.0009 3 0.4282\n",
      "1 3 0.6634\n",
      "2 3 0.6704\n",
      "3 3 0.6544\n",
      "4 3 0.6626\n",
      "5 3 0.657\n",
      "6 3 0.6426\n",
      "7 3 0.65\n",
      "8 3 0.659\n",
      "9 3 0.6562\n",
      "10 3 0.654\n",
      "11 3 0.6452\n",
      "12 3 0.6536\n",
      "13 3 0.6522\n",
      "14 3 0.6524\n",
      "15 3 0.6418\n",
      "16 3 0.6546\n",
      "17 3 0.6514\n",
      "18 3 0.6558\n",
      "19 3 0.654\n",
      "0.1 4 0.6264\n",
      "0.2 4 0.6464\n",
      "0.3 4 0.6404\n",
      "0.4 4 0.6602\n",
      "0.5 4 0.644\n",
      "0.6 4 0.553\n",
      "0.7 4 0.5372\n",
      "0.8 4 0.6518\n",
      "0.9 4 0.6196\n",
      "0.01 4 0.6046\n",
      "0.02 4 0.634\n",
      "0.03 4 0.6302\n",
      "0.04 4 0.6204\n",
      "0.05 4 0.6288\n",
      "0.06 4 0.6572\n",
      "0.07 4 0.618\n",
      "0.08 4 0.6386\n",
      "0.09 4 0.667\n",
      "0.001 4 0.4136\n",
      "0.002 4 0.446\n",
      "0.003 4 0.4884\n",
      "0.004 4 0.5834\n",
      "0.005 4 0.51\n",
      "0.006 4 0.5502\n",
      "0.007 4 0.5938\n",
      "0.008 4 0.5618\n",
      "0.009 4 0.5822\n",
      "0.0001 4 0.4\n",
      "0.0002 4 0.4\n",
      "0.0003 4 0.4\n",
      "0.0004 4 0.4\n",
      "0.0005 4 0.4\n",
      "0.0006 4 0.4004\n",
      "0.0007 4 0.413\n",
      "0.0008 4 0.4114\n",
      "0.0009 4 0.4102\n",
      "1 4 0.6664\n",
      "2 4 0.661\n",
      "3 4 0.6352\n",
      "4 4 0.6598\n",
      "5 4 0.5634\n",
      "6 4 0.6644\n",
      "7 4 0.6112\n",
      "8 4 0.5418\n",
      "9 4 0.6388\n",
      "10 4 0.6562\n",
      "11 4 0.6496\n",
      "12 4 0.5966\n",
      "13 4 0.652\n",
      "14 4 0.6116\n",
      "15 4 0.66\n",
      "16 4 0.659\n",
      "17 4 0.6462\n",
      "18 4 0.6232\n",
      "19 4 0.6536\n",
      "0.1 5 0.6582\n",
      "0.2 5 0.6678\n",
      "0.3 5 0.5918\n",
      "0.4 5 0.6776\n",
      "0.5 5 0.5984\n",
      "0.6 5 0.5314\n",
      "0.7 5 0.5734\n",
      "0.8 5 0.654\n",
      "0.9 5 0.6096\n",
      "0.01 5 0.5922\n",
      "0.02 5 0.6036\n",
      "0.03 5 0.631\n",
      "0.04 5 0.6288\n",
      "0.05 5 0.6378\n",
      "0.06 5 0.6186\n",
      "0.07 5 0.658\n",
      "0.08 5 0.6068\n",
      "0.09 5 0.6636\n",
      "0.001 5 0.4192\n",
      "0.002 5 0.5102\n",
      "0.003 5 0.5294\n",
      "0.004 5 0.5888\n",
      "0.005 5 0.5098\n",
      "0.006 5 0.5324\n",
      "0.007 5 0.5304\n",
      "0.008 5 0.5374\n",
      "0.009 5 0.598\n",
      "0.0001 5 0.4\n",
      "0.0002 5 0.4\n",
      "0.0003 5 0.4\n",
      "0.0004 5 0.4\n",
      "0.0005 5 0.4\n",
      "0.0006 5 0.4054\n",
      "0.0007 5 0.4142\n",
      "0.0008 5 0.4152\n",
      "0.0009 5 0.4134\n",
      "1 5 0.659\n",
      "2 5 0.5244\n",
      "3 5 0.667\n",
      "4 5 0.6658\n",
      "5 5 0.6128\n",
      "6 5 0.6396\n",
      "7 5 0.582\n",
      "8 5 0.6616\n",
      "9 5 0.6136\n",
      "10 5 0.6372\n",
      "11 5 0.6294\n",
      "12 5 0.6492\n",
      "13 5 0.601\n",
      "14 5 0.6038\n",
      "15 5 0.6216\n",
      "16 5 0.6212\n",
      "17 5 0.5336\n",
      "18 5 0.6462\n",
      "19 5 0.6634\n",
      "0.1 6 0.67\n",
      "0.2 6 0.6756\n",
      "0.3 6 0.6016\n",
      "0.4 6 0.6724\n",
      "0.5 6 0.668\n",
      "0.6 6 0.4884\n",
      "0.7 6 0.653\n",
      "0.8 6 0.603\n",
      "0.9 6 0.6222\n",
      "0.01 6 0.5958\n",
      "0.02 6 0.6458\n",
      "0.03 6 0.6186\n",
      "0.04 6 0.5808\n",
      "0.05 6 0.632\n",
      "0.06 6 0.6204\n",
      "0.07 6 0.6002\n",
      "0.08 6 0.661\n",
      "0.09 6 0.651\n",
      "0.001 6 0.4158\n",
      "0.002 6 0.4288\n",
      "0.003 6 0.5496\n",
      "0.004 6 0.5368\n",
      "0.005 6 0.4862\n",
      "0.006 6 0.5716\n",
      "0.007 6 0.5914\n",
      "0.008 6 0.5652\n",
      "0.009 6 0.5926\n",
      "0.0001 6 0.4\n",
      "0.0002 6 0.4\n",
      "0.0003 6 0.4\n",
      "0.0004 6 0.4004\n",
      "0.0005 6 0.4032\n",
      "0.0006 6 0.4\n",
      "0.0007 6 0.421\n",
      "0.0008 6 0.4094\n",
      "0.0009 6 0.4164\n",
      "1 6 0.6468\n",
      "2 6 0.6082\n",
      "3 6 0.6704\n",
      "4 6 0.6518\n",
      "5 6 0.665\n",
      "6 6 0.578\n",
      "7 6 0.6386\n",
      "8 6 0.5728\n",
      "9 6 0.5524\n",
      "10 6 0.6242\n",
      "11 6 0.6628\n",
      "12 6 0.6562\n",
      "13 6 0.6286\n",
      "14 6 0.6326\n",
      "15 6 0.6166\n",
      "16 6 0.6698\n",
      "17 6 0.549\n",
      "18 6 0.663\n",
      "19 6 0.601\n",
      "0.1 7 0.6008\n",
      "0.2 7 0.6636\n",
      "0.3 7 0.6204\n",
      "0.4 7 0.581\n",
      "0.5 7 0.6474\n",
      "0.6 7 0.5042\n",
      "0.7 7 0.6602\n",
      "0.8 7 0.5504\n",
      "0.9 7 0.6534\n",
      "0.01 7 0.5902\n",
      "0.02 7 0.5798\n",
      "0.03 7 0.652\n",
      "0.04 7 0.638\n",
      "0.05 7 0.6256\n",
      "0.06 7 0.6358\n",
      "0.07 7 0.669\n",
      "0.08 7 0.6642\n",
      "0.09 7 0.6596\n",
      "0.001 7 0.4226\n",
      "0.002 7 0.4996\n",
      "0.003 7 0.4948\n",
      "0.004 7 0.4832\n",
      "0.005 7 0.5978\n",
      "0.006 7 0.4724\n",
      "0.007 7 0.579\n",
      "0.008 7 0.5078\n",
      "0.009 7 0.6084\n",
      "0.0001 7 0.4\n",
      "0.0002 7 0.4\n",
      "0.0003 7 0.4\n",
      "0.0004 7 0.4\n",
      "0.0005 7 0.4046\n",
      "0.0006 7 0.4\n",
      "0.0007 7 0.4134\n",
      "0.0008 7 0.4146\n",
      "0.0009 7 0.4234\n",
      "1 7 0.657\n",
      "2 7 0.6134\n",
      "3 7 0.6284\n",
      "4 7 0.6222\n",
      "5 7 0.645\n",
      "6 7 0.5228\n",
      "7 7 0.6598\n",
      "8 7 0.6446\n",
      "9 7 0.4858\n",
      "10 7 0.5856\n",
      "11 7 0.639\n",
      "12 7 0.5302\n",
      "13 7 0.5922\n",
      "14 7 0.545\n",
      "15 7 0.6586\n",
      "16 7 0.6676\n",
      "17 7 0.6112\n",
      "18 7 0.5496\n",
      "19 7 0.5662\n",
      "0.1 8 0.669\n",
      "0.2 8 0.6606\n",
      "0.3 8 0.63\n",
      "0.4 8 0.671\n",
      "0.5 8 0.6768\n",
      "0.6 8 0.66\n",
      "0.7 8 0.589\n",
      "0.8 8 0.6486\n",
      "0.9 8 0.647\n",
      "0.01 8 0.6072\n",
      "0.02 8 0.6484\n",
      "0.03 8 0.6178\n",
      "0.04 8 0.6502\n",
      "0.05 8 0.6656\n",
      "0.06 8 0.6612\n",
      "0.07 8 0.658\n",
      "0.08 8 0.6132\n",
      "0.09 8 0.67\n",
      "0.001 8 0.418\n",
      "0.002 8 0.4332\n",
      "0.003 8 0.5108\n",
      "0.004 8 0.5148\n",
      "0.005 8 0.5324\n",
      "0.006 8 0.5554\n",
      "0.007 8 0.5448\n",
      "0.008 8 0.5566\n",
      "0.009 8 0.5914\n",
      "0.0001 8 0.4\n",
      "0.0002 8 0.4\n",
      "0.0003 8 0.4\n",
      "0.0004 8 0.4\n",
      "0.0005 8 0.4006\n",
      "0.0006 8 0.4146\n",
      "0.0007 8 0.4106\n",
      "0.0008 8 0.4086\n",
      "0.0009 8 0.4134\n",
      "1 8 0.615\n",
      "2 8 0.6574\n",
      "3 8 0.6418\n",
      "4 8 0.5398\n",
      "5 8 0.653\n",
      "6 8 0.6622\n",
      "7 8 0.4766\n",
      "8 8 0.6496\n",
      "9 8 0.6488\n",
      "10 8 0.5952\n",
      "11 8 0.5088\n",
      "12 8 0.6218\n",
      "13 8 0.6132\n",
      "14 8 0.6638\n",
      "15 8 0.6698\n",
      "16 8 0.6226\n",
      "17 8 0.5664\n",
      "18 8 0.645\n",
      "19 8 0.6228\n",
      "0.1 9 0.6216\n",
      "0.2 9 0.654\n",
      "0.3 9 0.5396\n",
      "0.4 9 0.6102\n",
      "0.5 9 0.6224\n",
      "0.6 9 0.664\n",
      "0.7 9 0.4174\n",
      "0.8 9 0.6588\n",
      "0.9 9 0.6416\n",
      "0.01 9 0.5828\n",
      "0.02 9 0.6262\n",
      "0.03 9 0.6488\n",
      "0.04 9 0.639\n",
      "0.05 9 0.6014\n",
      "0.06 9 0.662\n",
      "0.07 9 0.639\n",
      "0.08 9 0.6432\n",
      "0.09 9 0.6604\n",
      "0.001 9 0.414\n",
      "0.002 9 0.4596\n",
      "0.003 9 0.5234\n",
      "0.004 9 0.5478\n",
      "0.005 9 0.518\n",
      "0.006 9 0.5826\n",
      "0.007 9 0.5486\n",
      "0.008 9 0.5646\n",
      "0.009 9 0.604\n",
      "0.0001 9 0.4\n",
      "0.0002 9 0.4\n",
      "0.0003 9 0.4\n",
      "0.0004 9 0.4\n",
      "0.0005 9 0.4\n",
      "0.0006 9 0.4118\n",
      "0.0007 9 0.411\n",
      "0.0008 9 0.4154\n",
      "0.0009 9 0.4098\n",
      "1 9 0.6484\n",
      "2 9 0.6466\n",
      "3 9 0.6098\n",
      "4 9 0.6282\n",
      "5 9 0.626\n",
      "6 9 0.6134\n",
      "7 9 0.662\n",
      "8 9 0.6382\n",
      "9 9 0.6474\n",
      "10 9 0.6256\n",
      "11 9 0.6314\n",
      "12 9 0.5464\n",
      "13 9 0.6626\n",
      "14 9 0.6544\n",
      "15 9 0.5816\n",
      "16 9 0.6536\n",
      "17 9 0.6012\n",
      "18 9 0.632\n",
      "19 9 0.623\n",
      "0.1 10 0.6562\n",
      "0.2 10 0.6664\n",
      "0.3 10 0.6684\n",
      "0.4 10 0.6352\n",
      "0.5 10 0.5766\n",
      "0.6 10 0.6624\n",
      "0.7 10 0.6584\n",
      "0.8 10 0.6372\n",
      "0.9 10 0.5668\n",
      "0.01 10 0.6164\n",
      "0.02 10 0.5896\n",
      "0.03 10 0.602\n",
      "0.04 10 0.633\n",
      "0.05 10 0.6316\n",
      "0.06 10 0.6584\n",
      "0.07 10 0.6284\n",
      "0.08 10 0.6624\n",
      "0.09 10 0.5566\n",
      "0.001 10 0.4166\n",
      "0.002 10 0.4446\n",
      "0.003 10 0.4572\n",
      "0.004 10 0.5614\n",
      "0.005 10 0.5804\n",
      "0.006 10 0.5274\n",
      "0.007 10 0.5794\n",
      "0.008 10 0.4844\n",
      "0.009 10 0.597\n",
      "0.0001 10 0.4\n",
      "0.0002 10 0.4\n",
      "0.0003 10 0.4\n",
      "0.0004 10 0.4\n",
      "0.0005 10 0.4\n",
      "0.0006 10 0.4\n",
      "0.0007 10 0.412\n",
      "0.0008 10 0.4146\n",
      "0.0009 10 0.4166\n",
      "1 10 0.5898\n",
      "2 10 0.6618\n",
      "3 10 0.601\n",
      "4 10 0.5518\n",
      "5 10 0.6456\n",
      "6 10 0.645\n",
      "7 10 0.6632\n",
      "8 10 0.5552\n",
      "9 10 0.622\n",
      "10 10 0.6244\n",
      "11 10 0.5624\n",
      "12 10 0.6624\n",
      "13 10 0.632\n",
      "14 10 0.5998\n",
      "15 10 0.598\n",
      "16 10 0.5608\n",
      "17 10 0.6332\n",
      "18 10 0.646\n",
      "19 10 0.6614\n",
      "0.1 11 0.6354\n",
      "0.2 11 0.5986\n",
      "0.3 11 0.6084\n",
      "0.4 11 0.5928\n",
      "0.5 11 0.506\n",
      "0.6 11 0.6484\n",
      "0.7 11 0.6242\n",
      "0.8 11 0.627\n",
      "0.9 11 0.6368\n",
      "0.01 11 0.5906\n",
      "0.02 11 0.639\n",
      "0.03 11 0.6114\n",
      "0.04 11 0.6414\n",
      "0.05 11 0.6066\n",
      "0.06 11 0.6386\n",
      "0.07 11 0.6492\n",
      "0.08 11 0.6524\n",
      "0.09 11 0.6736\n",
      "0.001 11 0.4146\n",
      "0.002 11 0.4604\n",
      "0.003 11 0.458\n",
      "0.004 11 0.5106\n",
      "0.005 11 0.5158\n",
      "0.006 11 0.5908\n",
      "0.007 11 0.5662\n",
      "0.008 11 0.568\n",
      "0.009 11 0.5624\n",
      "0.0001 11 0.4\n",
      "0.0002 11 0.4\n",
      "0.0003 11 0.4\n",
      "0.0004 11 0.4\n",
      "0.0005 11 0.4022\n",
      "0.0006 11 0.4\n",
      "0.0007 11 0.4134\n",
      "0.0008 11 0.4098\n",
      "0.0009 11 0.422\n",
      "1 11 0.6342\n",
      "2 11 0.6034\n",
      "3 11 0.6404\n",
      "4 11 0.6642\n",
      "5 11 0.6118\n",
      "6 11 0.5658\n",
      "7 11 0.6344\n",
      "8 11 0.5612\n",
      "9 11 0.6682\n",
      "10 11 0.6154\n",
      "11 11 0.5716\n",
      "12 11 0.6036\n",
      "13 11 0.6322\n",
      "14 11 0.6184\n",
      "15 11 0.6598\n",
      "16 11 0.6636\n",
      "17 11 0.5986\n",
      "18 11 0.6704\n",
      "19 11 0.6438\n",
      "0.1 12 0.6696\n",
      "0.2 12 0.6312\n",
      "0.3 12 0.6466\n",
      "0.4 12 0.6444\n",
      "0.5 12 0.6242\n",
      "0.6 12 0.6084\n",
      "0.7 12 0.6622\n",
      "0.8 12 0.6446\n",
      "0.9 12 0.6232\n",
      "0.01 12 0.6112\n",
      "0.02 12 0.6204\n",
      "0.03 12 0.62\n",
      "0.04 12 0.6432\n",
      "0.05 12 0.6578\n",
      "0.06 12 0.608\n",
      "0.07 12 0.5952\n",
      "0.08 12 0.6554\n",
      "0.09 12 0.6436\n",
      "0.001 12 0.4308\n",
      "0.002 12 0.4538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003 12 0.5658\n",
      "0.004 12 0.4638\n",
      "0.005 12 0.533\n",
      "0.006 12 0.5004\n",
      "0.007 12 0.5564\n",
      "0.008 12 0.5862\n",
      "0.009 12 0.5672\n",
      "0.0001 12 0.4\n",
      "0.0002 12 0.4\n",
      "0.0003 12 0.4\n",
      "0.0004 12 0.4\n",
      "0.0005 12 0.4006\n",
      "0.0006 12 0.4054\n",
      "0.0007 12 0.4106\n",
      "0.0008 12 0.4114\n",
      "0.0009 12 0.4174\n",
      "1 12 0.6494\n",
      "2 12 0.6632\n",
      "3 12 0.5682\n",
      "4 12 0.6548\n",
      "5 12 0.663\n",
      "6 12 0.5794\n",
      "7 12 0.571\n",
      "8 12 0.6174\n",
      "9 12 0.5796\n",
      "10 12 0.6586\n",
      "11 12 0.6606\n",
      "12 12 0.5928\n",
      "13 12 0.6158\n",
      "14 12 0.5442\n",
      "15 12 0.6352\n",
      "16 12 0.6516\n",
      "17 12 0.5118\n",
      "18 12 0.6348\n",
      "19 12 0.6432\n",
      "0.1 13 0.6266\n",
      "0.2 13 0.6432\n",
      "0.3 13 0.6668\n",
      "0.4 13 0.6028\n",
      "0.5 13 0.6178\n",
      "0.6 13 0.6588\n",
      "0.7 13 0.535\n",
      "0.8 13 0.6112\n",
      "0.9 13 0.5696\n",
      "0.01 13 0.6238\n",
      "0.02 13 0.6336\n",
      "0.03 13 0.6358\n",
      "0.04 13 0.6368\n",
      "0.05 13 0.6422\n",
      "0.06 13 0.6548\n",
      "0.07 13 0.5304\n",
      "0.08 13 0.6252\n",
      "0.09 13 0.6572\n",
      "0.001 13 0.4234\n",
      "0.002 13 0.4232\n",
      "0.003 13 0.5162\n",
      "0.004 13 0.5356\n",
      "0.005 13 0.4748\n",
      "0.006 13 0.5358\n",
      "0.007 13 0.612\n",
      "0.008 13 0.5884\n",
      "0.009 13 0.5816\n",
      "0.0001 13 0.4\n",
      "0.0002 13 0.4\n",
      "0.0003 13 0.4\n",
      "0.0004 13 0.4\n",
      "0.0005 13 0.4006\n",
      "0.0006 13 0.4\n",
      "0.0007 13 0.4126\n",
      "0.0008 13 0.4116\n",
      "0.0009 13 0.4164\n",
      "1 13 0.5348\n",
      "2 13 0.632\n",
      "3 13 0.6222\n",
      "4 13 0.576\n",
      "5 13 0.5684\n",
      "6 13 0.6064\n",
      "7 13 0.6314\n",
      "8 13 0.6366\n",
      "9 13 0.6072\n",
      "10 13 0.62\n",
      "11 13 0.5516\n",
      "12 13 0.552\n",
      "13 13 0.6386\n",
      "14 13 0.6518\n",
      "15 13 0.6072\n",
      "16 13 0.5776\n",
      "17 13 0.6138\n",
      "18 13 0.654\n",
      "19 13 0.648\n",
      "0.1 14 0.6468\n",
      "0.2 14 0.6556\n",
      "0.3 14 0.6492\n",
      "0.4 14 0.6742\n",
      "0.5 14 0.6082\n",
      "0.6 14 0.5918\n",
      "0.7 14 0.6648\n",
      "0.8 14 0.6634\n",
      "0.9 14 0.6568\n",
      "0.01 14 0.562\n",
      "0.02 14 0.561\n",
      "0.03 14 0.647\n",
      "0.04 14 0.6048\n",
      "0.05 14 0.6278\n",
      "0.06 14 0.6616\n",
      "0.07 14 0.5766\n",
      "0.08 14 0.665\n",
      "0.09 14 0.6518\n",
      "0.001 14 0.435\n",
      "0.002 14 0.4324\n",
      "0.003 14 0.4848\n",
      "0.004 14 0.5614\n",
      "0.005 14 0.4494\n",
      "0.006 14 0.5388\n",
      "0.007 14 0.5956\n",
      "0.008 14 0.6004\n",
      "0.009 14 0.5504\n",
      "0.0001 14 0.4\n",
      "0.0002 14 0.4\n",
      "0.0003 14 0.4\n",
      "0.0004 14 0.4\n",
      "0.0005 14 0.4006\n",
      "0.0006 14 0.4046\n",
      "0.0007 14 0.4098\n",
      "0.0008 14 0.4112\n",
      "0.0009 14 0.4164\n",
      "1 14 0.5562\n",
      "2 14 0.6186\n",
      "3 14 0.6098\n",
      "4 14 0.6428\n",
      "5 14 0.6268\n",
      "6 14 0.6278\n",
      "7 14 0.5214\n",
      "8 14 0.631\n",
      "9 14 0.5486\n",
      "10 14 0.5818\n",
      "11 14 0.5954\n",
      "12 14 0.6\n",
      "13 14 0.632\n",
      "14 14 0.5898\n",
      "15 14 0.6474\n",
      "16 14 0.5872\n",
      "17 14 0.6434\n",
      "18 14 0.6584\n",
      "19 14 0.6598\n",
      "0.1 15 0.594\n",
      "0.2 15 0.6274\n",
      "0.3 15 0.6394\n",
      "0.4 15 0.5544\n",
      "0.5 15 0.6568\n",
      "0.6 15 0.6286\n",
      "0.7 15 0.6542\n",
      "0.8 15 0.4462\n",
      "0.9 15 0.615\n",
      "0.01 15 0.5738\n",
      "0.02 15 0.619\n",
      "0.03 15 0.6518\n",
      "0.04 15 0.6228\n",
      "0.05 15 0.6424\n",
      "0.06 15 0.6422\n",
      "0.07 15 0.565\n",
      "0.08 15 0.662\n",
      "0.09 15 0.5838\n",
      "0.001 15 0.4242\n",
      "0.002 15 0.4714\n",
      "0.003 15 0.496\n",
      "0.004 15 0.5076\n",
      "0.005 15 0.5638\n",
      "0.006 15 0.5944\n",
      "0.007 15 0.5356\n",
      "0.008 15 0.6072\n",
      "0.009 15 0.5834\n",
      "0.0001 15 0.4\n",
      "0.0002 15 0.4\n",
      "0.0003 15 0.4\n",
      "0.0004 15 0.4\n",
      "0.0005 15 0.4\n",
      "0.0006 15 0.417\n",
      "0.0007 15 0.4086\n",
      "0.0008 15 0.4142\n",
      "0.0009 15 0.413\n",
      "1 15 0.6616\n",
      "2 15 0.6558\n",
      "3 15 0.4254\n",
      "4 15 0.6606\n",
      "5 15 0.645\n",
      "6 15 0.636\n",
      "7 15 0.639\n",
      "8 15 0.5722\n",
      "9 15 0.5864\n",
      "10 15 0.6384\n",
      "11 15 0.6342\n",
      "12 15 0.6542\n",
      "13 15 0.63\n",
      "14 15 0.657\n",
      "15 15 0.6352\n",
      "16 15 0.5838\n",
      "17 15 0.658\n",
      "18 15 0.664\n",
      "19 15 0.6188\n",
      "0.1 16 0.5652\n",
      "0.2 16 0.6712\n",
      "0.3 16 0.6742\n",
      "0.4 16 0.6506\n",
      "0.5 16 0.6452\n",
      "0.6 16 0.6354\n",
      "0.7 16 0.6684\n",
      "0.8 16 0.6328\n",
      "0.9 16 0.603\n",
      "0.01 16 0.5624\n",
      "0.02 16 0.6334\n",
      "0.03 16 0.6516\n",
      "0.04 16 0.6238\n",
      "0.05 16 0.667\n",
      "0.06 16 0.6642\n",
      "0.07 16 0.6598\n",
      "0.08 16 0.6722\n",
      "0.09 16 0.6426\n",
      "0.001 16 0.415\n",
      "0.002 16 0.4276\n",
      "0.003 16 0.4614\n",
      "0.004 16 0.5066\n",
      "0.005 16 0.4996\n",
      "0.006 16 0.603\n",
      "0.007 16 0.5046\n",
      "0.008 16 0.5296\n",
      "0.009 16 0.6132\n",
      "0.0001 16 0.4\n",
      "0.0002 16 0.4\n",
      "0.0003 16 0.4\n",
      "0.0004 16 0.4\n",
      "0.0005 16 0.4134\n",
      "0.0006 16 0.4104\n",
      "0.0007 16 0.4132\n",
      "0.0008 16 0.418\n",
      "0.0009 16 0.4234\n",
      "1 16 0.6032\n",
      "2 16 0.5644\n",
      "3 16 0.6474\n",
      "4 16 0.6078\n",
      "5 16 0.586\n",
      "6 16 0.6172\n",
      "7 16 0.6544\n",
      "8 16 0.6358\n",
      "9 16 0.633\n",
      "10 16 0.5856\n",
      "11 16 0.6096\n",
      "12 16 0.6342\n",
      "13 16 0.6162\n",
      "14 16 0.6126\n",
      "15 16 0.5238\n",
      "16 16 0.6506\n",
      "17 16 0.6506\n",
      "18 16 0.654\n",
      "19 16 0.6372\n",
      "0.1 17 0.5472\n",
      "0.2 17 0.573\n",
      "0.3 17 0.5942\n",
      "0.4 17 0.5538\n",
      "0.5 17 0.6452\n",
      "0.6 17 0.601\n",
      "0.7 17 0.5406\n",
      "0.8 17 0.6588\n",
      "0.9 17 0.5308\n",
      "0.01 17 0.6014\n",
      "0.02 17 0.6316\n",
      "0.03 17 0.635\n",
      "0.04 17 0.6212\n",
      "0.05 17 0.6622\n",
      "0.06 17 0.6602\n",
      "0.07 17 0.6602\n",
      "0.08 17 0.5952\n",
      "0.09 17 0.6722\n",
      "0.001 17 0.4162\n",
      "0.002 17 0.4386\n",
      "0.003 17 0.4514\n",
      "0.004 17 0.5156\n",
      "0.005 17 0.5958\n",
      "0.006 17 0.5942\n",
      "0.007 17 0.5468\n",
      "0.008 17 0.6096\n",
      "0.009 17 0.5902\n",
      "0.0001 17 0.4\n",
      "0.0002 17 0.4\n",
      "0.0003 17 0.4\n",
      "0.0004 17 0.4\n",
      "0.0005 17 0.4\n",
      "0.0006 17 0.4\n",
      "0.0007 17 0.4112\n",
      "0.0008 17 0.4152\n",
      "0.0009 17 0.4204\n",
      "1 17 0.6504\n",
      "2 17 0.661\n",
      "3 17 0.5608\n",
      "4 17 0.6092\n",
      "5 17 0.5924\n",
      "6 17 0.5144\n",
      "7 17 0.6618\n",
      "8 17 0.6516\n",
      "9 17 0.6462\n",
      "10 17 0.5754\n",
      "11 17 0.6636\n",
      "12 17 0.6398\n",
      "13 17 0.6562\n",
      "14 17 0.6556\n",
      "15 17 0.6536\n",
      "16 17 0.6628\n",
      "17 17 0.571\n",
      "18 17 0.6352\n",
      "19 17 0.6422\n",
      "0.1 18 0.6504\n",
      "0.2 18 0.6734\n",
      "0.3 18 0.6282\n",
      "0.4 18 0.63\n",
      "0.5 18 0.6728\n",
      "0.6 18 0.6564\n",
      "0.7 18 0.6308\n",
      "0.8 18 0.565\n",
      "0.9 18 0.6688\n",
      "0.01 18 0.568\n",
      "0.02 18 0.6176\n",
      "0.03 18 0.6304\n",
      "0.04 18 0.595\n",
      "0.05 18 0.621\n",
      "0.06 18 0.6684\n",
      "0.07 18 0.629\n",
      "0.08 18 0.6128\n",
      "0.09 18 0.6358\n",
      "0.001 18 0.4796\n",
      "0.002 18 0.448\n",
      "0.003 18 0.4832\n",
      "0.004 18 0.572\n",
      "0.005 18 0.5982\n",
      "0.006 18 0.5388\n",
      "0.007 18 0.4972\n",
      "0.008 18 0.5918\n",
      "0.009 18 0.6102\n",
      "0.0001 18 0.4\n",
      "0.0002 18 0.4\n",
      "0.0003 18 0.4\n",
      "0.0004 18 0.4\n",
      "0.0005 18 0.4\n",
      "0.0006 18 0.4006\n",
      "0.0007 18 0.4112\n",
      "0.0008 18 0.4118\n",
      "0.0009 18 0.4176\n",
      "1 18 0.672\n",
      "2 18 0.669\n",
      "3 18 0.6154\n",
      "4 18 0.6482\n",
      "5 18 0.618\n",
      "6 18 0.5222\n",
      "7 18 0.6502\n",
      "8 18 0.6394\n",
      "9 18 0.5134\n",
      "10 18 0.6466\n",
      "11 18 0.6052\n",
      "12 18 0.5518\n",
      "13 18 0.582\n",
      "14 18 0.5736\n",
      "15 18 0.6116\n",
      "16 18 0.646\n",
      "17 18 0.6638\n",
      "18 18 0.6574\n",
      "19 18 0.5552\n",
      "0.1 19 0.544\n",
      "0.2 19 0.663\n",
      "0.3 19 0.6612\n",
      "0.4 19 0.6634\n",
      "0.5 19 0.5292\n",
      "0.6 19 0.6154\n",
      "0.7 19 0.659\n",
      "0.8 19 0.6018\n",
      "0.9 19 0.6566\n",
      "0.01 19 0.5508\n",
      "0.02 19 0.6318\n",
      "0.03 19 0.6422\n",
      "0.04 19 0.635\n",
      "0.05 19 0.6174\n",
      "0.06 19 0.654\n",
      "0.07 19 0.6464\n",
      "0.08 19 0.6482\n",
      "0.09 19 0.6586\n",
      "0.001 19 0.4158\n",
      "0.002 19 0.4306\n",
      "0.003 19 0.459\n",
      "0.004 19 0.5816\n",
      "0.005 19 0.5676\n",
      "0.006 19 0.5316\n",
      "0.007 19 0.609\n",
      "0.008 19 0.5846\n",
      "0.009 19 0.592\n",
      "0.0001 19 0.4\n",
      "0.0002 19 0.4\n",
      "0.0003 19 0.4\n",
      "0.0004 19 0.4\n",
      "0.0005 19 0.4\n",
      "0.0006 19 0.4\n",
      "0.0007 19 0.414\n",
      "0.0008 19 0.4112\n",
      "0.0009 19 0.4112\n",
      "1 19 0.666\n",
      "2 19 0.6702\n",
      "3 19 0.6302\n",
      "4 19 0.6686\n",
      "5 19 0.6542\n",
      "6 19 0.6202\n",
      "7 19 0.5432\n",
      "8 19 0.6322\n",
      "9 19 0.6074\n",
      "10 19 0.6252\n",
      "11 19 0.6146\n",
      "12 19 0.6314\n",
      "13 19 0.661\n",
      "14 19 0.6404\n",
      "15 19 0.6396\n",
      "16 19 0.6568\n",
      "17 19 0.5758\n",
      "18 19 0.6516\n",
      "19 19 0.6608\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(analyzer='word', norm='l2')\n",
    "xtrain = tfid.fit_transform(train_data)\n",
    "xvalidation = tfid.transform(validation_data)\n",
    "xtest = tfid.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for tolValue in alphaList:\n",
    "    for cValue in alphaList:\n",
    "        svc = LinearSVC(C = cValue, tol = tolValue)\n",
    "        svc.fit(xtrain, ytrain)\n",
    "        predicted = svc.predict(xvalidation)\n",
    "        print(cValue, tolValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c5fea11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.6886\n",
      "0.31 0.6894\n",
      "0.32 0.6896\n",
      "0.33 0.689\n",
      "0.34 0.6898\n",
      "0.35 0.6894\n",
      "0.36 0.6888\n",
      "0.37 0.6892\n",
      "0.38 0.6886\n",
      "0.39 0.6882\n",
      "0.4 0.6882\n",
      "0.41 0.6892\n",
      "0.42 0.6888\n",
      "0.43 0.6894\n",
      "0.44 0.6888\n",
      "0.45 0.6886\n",
      "0.46 0.6886\n",
      "0.47 0.6886\n",
      "0.48 0.6892\n",
      "0.49 0.6892\n",
      "0.5 0.6886\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "newList = [0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5]\n",
    "\n",
    "for alphaValue in newList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5b0db0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 0.689\n",
      "0.331 0.689\n",
      "0.332 0.6892\n",
      "0.333 0.6892\n",
      "0.334 0.6892\n",
      "0.335 0.6894\n",
      "0.336 0.6894\n",
      "0.337 0.6894\n",
      "0.338 0.6894\n",
      "0.339 0.6896\n",
      "0.34 0.6898\n",
      "0.341 0.6898\n",
      "0.342 0.6896\n",
      "0.343 0.6894\n",
      "0.344 0.6894\n",
      "0.345 0.6894\n",
      "0.346 0.6896\n",
      "0.347 0.6896\n",
      "0.348 0.6896\n",
      "0.349 0.6896\n",
      "0.35 0.6894\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "newList = [0.33, 0.331, 0.332, 0.333, 0.334, 0.335, 0.336, 0.337, 0.338, 0.339, 0.34, 0.341, 0.342, 0.343, 0.344, 0.345, 0.346, 0.347, 0.348, 0.349, 0.35]\n",
    "\n",
    "for alphaValue in newList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b671f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6786\n",
      "0.2 0.6854\n",
      "0.3 0.6888\n",
      "0.4 0.6876\n",
      "0.5 0.6886\n",
      "0.6 0.688\n",
      "0.7 0.6898\n",
      "0.8 0.6896\n",
      "0.9 0.6894\n",
      "0.01 0.6696\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14708/2942096356.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malphaValue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malphaList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malphaValue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malphaValue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_effective_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m         \u001b[0mcheck_non_negative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    150\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(encoding = 'utf-8', strip_accents = 'ascii')\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79481333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3 0.6896\n",
      "0.31 0.6898\n",
      "0.32 0.69\n",
      "0.33 0.6904\n",
      "0.34 0.69\n",
      "0.35 0.6896\n",
      "0.36 0.6894\n",
      "0.37 0.6896\n",
      "0.38 0.6892\n",
      "0.39 0.6898\n",
      "0.4 0.6902\n",
      "0.41 0.6896\n",
      "0.42 0.69\n",
      "0.43 0.6902\n",
      "0.44 0.69\n",
      "0.45 0.6898\n",
      "0.46 0.6898\n",
      "0.47 0.6894\n",
      "0.48 0.689\n",
      "0.49 0.6886\n",
      "0.5 0.6884\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(encoding = 'str', strip_accents = 'unicode')\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "newList = [0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5]\n",
    "\n",
    "for alphaValue in newList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a5bf0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34 0.6898\n",
      "5000\n",
      "[2 1 2 ... 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "mnb = MultinomialNB(alpha = 0.34)\n",
    "mnb.fit(xtrain, ytrain)\n",
    "predicted = mnb.predict(xvalidation)\n",
    "print(0.34, accuracy_score(predicted, yvalidation), sep = \" \")\n",
    "\n",
    "\n",
    "predictedMnbSubmission = mnb.predict(xtest)\n",
    "print(len(predictedMnbSubmission))\n",
    "print(predictedMnbSubmission)\n",
    "\n",
    "# output = open('data/test_labels.txt', 'w')\n",
    "# output.write('id,label\\n')\n",
    "# for i in range(len(test_ids)):\n",
    "#     output.write(str(test_ids[i]) + ',' + str(predictedMnbSubmission[i]) + '\\n')\n",
    "# output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d219896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34 0.876\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictedMnbSubmission' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1420/3234223919.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mpredictedSubmission\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictedMnbSubmission\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictedMnbSubmission\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictedMnbSubmission' is not defined"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data.append(validation_data))\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "newy = ytrain.append(yvalidation)\n",
    "\n",
    "mnb = MultinomialNB(alpha = 0.34)\n",
    "mnb.fit(xtrain, newy)\n",
    "predicted = mnb.predict(xvalidation)\n",
    "print(0.34, accuracy_score(predicted, yvalidation), sep = \" \")\n",
    "\n",
    "\n",
    "predictedSubmission = mnb.predict(xtest)\n",
    "print(len(predictedMnbSubmission))\n",
    "print(predictedMnbSubmission)\n",
    "\n",
    "output = open('data/test_labels.txt', 'w')\n",
    "output.write('id,label\\n')\n",
    "for i in range(len(test_ids)):\n",
    "    output.write(str(test_ids[i]) + ',' + str(predictedSubmission[i]) + '\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "235c40fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 0.8768\n",
      "5000\n",
      "[2 2 2 ... 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(encoding = 'str', strip_accents = 'unicode')\n",
    "\n",
    "xtrain = cv.fit_transform(train_data.append(validation_data))\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "newy = ytrain.append(yvalidation)\n",
    "\n",
    "mnb = MultinomialNB(alpha = 0.33)\n",
    "mnb.fit(xtrain, newy)\n",
    "predicted = mnb.predict(xvalidation)\n",
    "print(0.33, accuracy_score(predicted, yvalidation), sep = \" \")\n",
    "\n",
    "\n",
    "predictedMnbSubmission = mnb.predict(xtest)\n",
    "print(len(predictedMnbSubmission))\n",
    "print(predictedMnbSubmission)\n",
    "\n",
    "output = open('data/test_labels.txt', 'w')\n",
    "output.write('id,label\\n')\n",
    "for i in range(len(test_ids)):\n",
    "    output.write(str(test_ids[i]) + ',' + str(predictedMnbSubmission[i]) + '\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "770fb5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34 0.6488\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "mnb = Perceptron(alpha = 0.0001, penalty = 'elasticnet', l1_ratio = 0, eta0 = 0.001)\n",
    "mnb.fit(xtrain, ytrain)\n",
    "predicted = mnb.predict(xvalidation)\n",
    "print(0.34, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affb7c1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14708/1784520811.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mxtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mxvalidation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "mnb = MLPClassifier()\n",
    "mnb.fit(xtrain, ytrain)\n",
    "predicted = mnb.predict(xvalidation)\n",
    "print(0.34, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b0db4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34 0.6874\n",
      "5000\n",
      "[2 1 2 ... 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "mnb = MultinomialNB(alpha = 0.34, fit_prior = False)\n",
    "mnb.fit(xtrain, ytrain)\n",
    "predicted = mnb.predict(xvalidation)\n",
    "print(0.34, accuracy_score(predicted, yvalidation), sep = \" \")\n",
    "\n",
    "\n",
    "predictedMnbSubmission = mnb.predict(xtest)\n",
    "print(len(predictedMnbSubmission))\n",
    "print(predictedMnbSubmission)\n",
    "\n",
    "# output = open('data/test_labels.txt', 'w')\n",
    "# output.write('id,label\\n')\n",
    "# for i in range(len(test_ids)):\n",
    "#     output.write(str(test_ids[i]) + ',' + str(predictedMnbSubmission[i]) + '\\n')\n",
    "# output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be612e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 0.6872\n",
      "0.331 0.6872\n",
      "0.332 0.6872\n",
      "0.333 0.6872\n",
      "0.334 0.6872\n",
      "0.335 0.6874\n",
      "0.336 0.6874\n",
      "0.337 0.6874\n",
      "0.338 0.6874\n",
      "0.339 0.6876\n",
      "0.34 0.6874\n",
      "0.341 0.6872\n",
      "0.342 0.6874\n",
      "0.343 0.6874\n",
      "0.344 0.6874\n",
      "0.345 0.6874\n",
      "0.346 0.6874\n",
      "0.347 0.6872\n",
      "0.348 0.6872\n",
      "0.349 0.6872\n",
      "0.35 0.6872\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "newList = [0.33, 0.331, 0.332, 0.333, 0.334, 0.335, 0.336, 0.337, 0.338, 0.339, 0.34, 0.341, 0.342, 0.343, 0.344, 0.345, 0.346, 0.347, 0.348, 0.349, 0.35]\n",
    "\n",
    "for alphaValue in newList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue, fit_prior = False)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08700f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 0.6814\n",
      "0.2 0.683\n",
      "0.3 0.6876\n",
      "0.4 0.6872\n",
      "0.5 0.6862\n",
      "0.6 0.6854\n",
      "0.7 0.6846\n",
      "0.8 0.6848\n",
      "0.9 0.6862\n",
      "0.01 0.6684\n",
      "0.02 0.6726\n",
      "0.03 0.6732\n",
      "0.04 0.6748\n",
      "0.05 0.6766\n",
      "0.06 0.6772\n",
      "0.07 0.6786\n",
      "0.08 0.6798\n",
      "0.09 0.6808\n",
      "0.001 0.6578\n",
      "0.002 0.662\n",
      "0.003 0.6634\n",
      "0.004 0.663\n",
      "0.005 0.6644\n",
      "0.006 0.665\n",
      "0.007 0.6654\n",
      "0.008 0.6662\n",
      "0.009 0.6676\n",
      "0.0001 0.647\n",
      "0.0002 0.65\n",
      "0.0003 0.6516\n",
      "0.0004 0.6524\n",
      "0.0005 0.6538\n",
      "0.0006 0.6546\n",
      "0.0007 0.6554\n",
      "0.0008 0.657\n",
      "0.0009 0.657\n",
      "1 0.6838\n",
      "2 0.684\n",
      "3 0.6788\n",
      "4 0.6616\n",
      "5 0.645\n",
      "6 0.6244\n",
      "7 0.6118\n",
      "8 0.5956\n",
      "9 0.5804\n",
      "10 0.5672\n",
      "11 0.5572\n",
      "12 0.547\n",
      "13 0.5362\n",
      "14 0.5298\n",
      "15 0.5226\n",
      "16 0.516\n",
      "17 0.509\n",
      "18 0.5026\n",
      "19 0.499\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "for alphaValue in alphaList:\n",
    "    mnb = MultinomialNB(alpha = alphaValue, fit_prior = False)\n",
    "    mnb.fit(xtrain, ytrain)\n",
    "    predicted = mnb.predict(xvalidation)\n",
    "    print(alphaValue, accuracy_score(predicted, yvalidation), sep = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6855070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.624\n",
      "5000\n",
      "[2 2 1 ... 2 1 1]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "xtrain = cv.fit_transform(train_data)\n",
    "xvalidation = cv.transform(validation_data)\n",
    "xtest = cv.transform(test_data)\n",
    "\n",
    "xtrain = xtrain.toarray()\n",
    "xvalidation = xvalidation.toarray()\n",
    "xtest = xtest.toarray()\n",
    "\n",
    "svmClf = svm.SVC(C = 1, kernel = 'linear')\n",
    "svmClf.fit(xtrain, ytrain)\n",
    "predicted = svmClf.predict(xvalidation)\n",
    "print(accuracy_score(predicted, yvalidation))\n",
    "\n",
    "predictedMnbSubmission = svmClf.predict(xtest)\n",
    "print(len(predictedMnbSubmission))\n",
    "print(predictedMnbSubmission)\n",
    "\n",
    "output = open('data/test_labels.txt', 'w')\n",
    "output.write('id,label\\n')\n",
    "for i in range(len(test_ids)):\n",
    "    output.write(str(test_ids[i]) + ',' + str(predictedMnbSubmission[i]) + '\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f58c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
